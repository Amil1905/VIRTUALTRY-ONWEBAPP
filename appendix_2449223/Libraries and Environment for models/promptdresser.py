# -*- coding: utf-8 -*-
"""PromptDresser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jzbzQOMiltkhQbP6gW9VqVSfw6dPCvRl
"""

from google.colab import drive
drive.mount('/content/drive')

!git clone https://github.com/rlawjdghek/PromptDresser /content/drive/MyDrive/PromptDresser

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/PromptDresser

# Requirements.txt'i kontrol et
!cat requirements.txt | head -20

# T√ºm requirements'i y√ºkle
!pip install -r requirements.txt --force-reinstall

# √ñnemli paketlerin versiyonlarƒ±nƒ± kontrol et
!pip show diffusers transformers accelerate peft huggingface-hub | grep Version

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/PromptDresser

!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121

!pip install diffusers==0.25.0
!pip install accelerate==0.31.0
!pip install 'transformers>=4.25.1'

!pip install ftfy Jinja2 datasets wandb onnxruntime-gpu==1.19.2
!pip install omegaconf einops torchmetrics clean-fid scikit-image

!pip install opencv-python fvcore cloudpickle pycocotools av scipy peft

!pip install huggingface-hub==0.24.6

!apt-get update && apt-get install -y git-lfs
!git lfs install

# Commented out IPython magic to ensure Python compatibility.
#!mkdir -p pretrained_models
# %cd pretrained_models

!git clone https://huggingface.co/diffusers/stable-diffusion-xl-1.0-inpainting-0.1

!git clone https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0

!wget -O /content/drive/MyDrive/PromptDresser/pretrained_models/stable-diffusion-xl-base-1.0/sd_xl_base_1.0.safetensors "https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors"

!wget -O /content/drive/MyDrive/PromptDresser/pretrained_models/stable-diffusion-xl-1.0-inpainting-0.1/unet/diffusion_pytorch_model.fp16.safetensors \
"https://huggingface.co/diffusers/stable-diffusion-xl-1.0-inpainting-0.1/resolve/main/unet/diffusion_pytorch_model.fp16.safetensors"

!wget -O /content/drive/MyDrive/PromptDresser/pretrained_models/stable-diffusion-xl-1.0-inpainting-0.1/unet/diffusion_pytorch_model.safetensors \
"https://huggingface.co/diffusers/stable-diffusion-xl-1.0-inpainting-0.1/resolve/main/unet/diffusion_pytorch_model.safetensors"

!wget -O /content/drive/MyDrive/PromptDresser/pretrained_models/stable-diffusion-xl-base-1.0/sd_xl_base_1.0_0.9vae.safetensors \
"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0_0.9vae.safetensors"

!wget -O /content/drive/MyDrive/PromptDresser/pretrained_models/stable-diffusion-xl-base-1.0/unet/diffusion_pytorch_model.fp16.safetensors \
"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/unet/diffusion_pytorch_model.fp16.safetensors"

!wget -O /content/drive/MyDrive/PromptDresser/pretrained_models/stable-diffusion-xl-base-1.0/unet/diffusion_pytorch_model.safetensors \
"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/unet/diffusion_pytorch_model.safetensors"

!wget -O /content/drive/MyDrive/PromptDresser/pretrained_models/stable-diffusion-xl-base-1.0/unet/diffusion_flax_model.msgpack \
"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/unet/diffusion_flax_model.msgpack"

!wget -O /content/drive/MyDrive/PromptDresser/pretrained_models/stable-diffusion-xl-base-1.0/unet/openvino_model.bin \
"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/unet/openvino_model.bin"

!unzip -o /content/drive/MyDrive/PromptDresser/humanparsing.zip -d checkpoints/

!unzip -o /content/drive/MyDrive/PromptDresser/VITONHD.zip -d checkpoints/

!unzip -o /content/drive/MyDrive/PromptDresser/zalando-hd-resized.zip -d /content

!cp -r /content/test/* /content/drive/MyDrive/PromptDresser/test_coarse/

# üìÅ test_coarse i√ßindeki her ≈üeyi test_fine'a da kopyala
!cp -r /content/test/* /content/drive/MyDrive/PromptDresser/test_fine/

!cp -r /content/test/* /content/drive/MyDrive/PromptDresser/test_fine/

!unzip -o /content/drive/MyDrive/PromptDresser/VITONHD_mask.zip -d /content/

!git lfs install
!git clone https://huggingface.co/madebyollin/sdxl-vae-fp16-fix pretrained_models/sdxl-vae-fp16-fix

!rm -rf /content/drive/MyDrive/PromptDresser/test_coarse/agnostic-mask

# test_fine‚Äôteki eski agnostic-mask‚Äôi sil
!rm -rf /content/drive/MyDrive/PromptDresser/test_fine/agnostic-mask

!unzip -o /content/VITONHD_mask/test_coarse_agnostic_mask.zip -d /content/drive/MyDrive/PromptDresser/test_coarse/
!unzip -o /content/VITONHD_mask/test_fine_agnostic_mask.zip -d /content/drive/MyDrive/PromptDresser/test_fine/

# üßπ NumPy <2
!pip install numpy==1.26.4 --force-reinstall

# üßπ huggingface-hub tam s√ºr√ºme √ßek
!pip install huggingface-hub==0.24.6 --force-reinstall

# üî∑ PyTorch (CUDA 12.1)
!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121 --force-reinstall

# üî∑ diffusers ve accelerate (d√∂k√ºmantasyon s√ºr√ºm√º)
!pip install diffusers==0.25.0 --force-reinstall
!pip install accelerate==0.31.0 --force-reinstall
!pip install "transformers>=4.25.1" --force-reinstall

# üî∑ Diƒüer baƒüƒ±mlƒ±lƒ±klar
!pip install ftfy Jinja2 datasets wandb onnxruntime-gpu==1.19.2 omegaconf einops torchmetrics clean-fid scikit-image opencv-python fvcore cloudpickle pycocotools av scipy peft --force-reinstall

!pip install peft==0.4.0

!mkdir -p DATA/zalando-hd-resized

!mv /content/drive/MyDrive/PromptDresser/test_pairs.txt DATA/zalando-hd-resized/
!mv /content/drive/MyDrive/PromptDresser/test_fine DATA/zalando-hd-resized/
!mv /content/drive/MyDrive/PromptDresser/test_coarse DATA/zalando-hd-resized/

!pip freeze > requirements.txt

# Colab'da yapƒ±n:
!cp evaluation.py evaluation_backup.py    # backup
!cp enhancedevalutaion.py evaluation.py   # replace

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/PromptDresser

# √ñnce inference
!CUDA_VISIBLE_DEVICES=0 python inference.py \
  --config_p "./configs/VITONHD.yaml" \
  --pretrained_unet_path "./checkpoints/VITONHD/model/pytorch_model.bin" \
  --save_name base8 \
  --skip_unpaired \

# Sonra evaluation
!python evaluation.py \
  --gt_dir ./DATA/zalando-hd-resized/test_fine/image \
  --pred_dir ./sampled_images/base8/paired \
  --enhanced \
  --model_path ./checkpoints/VITONHD/model/pytorch_model.bin \
  --text_pairs_json test_gpt4o.json

!CUDA_VISIBLE_DEVICES=0 python inference.py \
  --config_p "./configs/VITONHD.yaml" \
  --pretrained_unet_path "./checkpoints/VITONHD/model/pytorch_model.bin" \
  --save_name test_enhancedd \
  --skip_unpaired \
  --s_idx 0 --e_idx 1

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/PromptDresser/

!python -c "import torch, torchvision, torchaudio, platform; \
print('torch', torch.__version__); \
print('torchvision', torchvision.__version__); \
print('torchaudio', torchaudio.__version__); \
print('CUDA available:', torch.cuda.is_available()); \
print('CUDA runtime:', torch.version.cuda); \
print('GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'); \
print('Python', platform.python_version())"

!CUDA_VISIBLE_DEVICES=0 python inference.py \
  --config_p "./configs/VITONHD.yaml" \
  --save_name "lora_20sample_styletest" \
  --skip_unpaired \
  --s_idx 0 --e_idx 1

!ngrok config add-authtoken "2ypag7GiYBHUMevVqG4aZRQ4ymR_552mLi7WreTtuN1dY64dM"

!pip install fastapi uvicorn pyngrok

!python3 prompt_dresser_api.py

from pyngrok import ngrok

public_url = ngrok.connect(8003)
print(public_url)

!uvicorn prompt_dresser_api:app --host 0.0.0.0 --port 8003

!pip install "transformers>=4.25.1,<4.27.0"

# ‚ö°Ô∏è PyTorch (CUDA 12.1)
!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121

# ‚ö°Ô∏è diffusers, accelerate, transformers, hub
!pip install diffusers==0.25.0
!pip install accelerate==0.31.0
!pip install "transformers>=4.25.1"
!pip install huggingface-hub==0.24.6

# ‚ö°Ô∏è Diƒüer baƒüƒ±mlƒ±lƒ±klar
!pip install ftfy
!pip install Jinja2
!pip install datasets
!pip install wandb
!pip install onnxruntime-gpu==1.19.2
!pip install omegaconf
!pip install einops
!pip install torchmetrics
!pip install clean-fid
!pip install scikit-image
!pip install opencv-python
!pip install fvcore
!pip install cloudpickle
!pip install pycocotools
!pip install av
!pip install scipy
!pip install peft

!pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu121 --force-reinstall

!pip uninstall -y torch torchvision torchaudio
!pip cache purge

!pip install torch==2.5.1+cu121 torchvision==0.20.1+cu121 torchaudio==2.5.1+cu121 \
  --index-url https://download.pytorch.org/whl/cu121

!pip uninstall -y transformers accelerate tokenizers

# Uyumlu ve g√ºncel s√ºr√ºmler (Py3.12 i√ßin hazƒ±r wheel‚Äôleri var)
!pip install transformers==4.44.2 accelerate==0.34.2 tokenizers==0.19.1

!pip install numpy==1.26.4 --force-reinstall

!pip install transformers==4.26.1 accelerate==0.21.0 --force-reinstall

!pip install huggingface-hub==0.24.6 --force-reinstall

!pip show diffusers | grep Version

!pip show accelerate | grep Version

!pip install -U accelerate

!pip install -U peft

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/drive/MyDrive/PromptDresser/promptdresser_style_dataset.py
# import torch
# from torch.utils.data import Dataset
# from PIL import Image
# import json
# import os
# from torchvision import transforms
# import numpy as np
# 
# class PromptDresserStyleDataset(Dataset):
#     """
#     PromptDresser Dataset with Style Support (Summer/Winter/Casual/Sport)
#     Using simple style keyword approach
#     """
#     def __init__(self, data_dir, mode='fine', image_size=512):
#         self.data_dir = data_dir
#         self.mode = mode
#         self.image_size = image_size
# 
#         self.pairs = self.load_pairs()
#         self.prompts = self.load_style_prompts()
# 
#         self.transform = transforms.Compose([
#             transforms.Resize((image_size, image_size)),
#             transforms.ToTensor(),
#             transforms.Normalize([0.5], [0.5])
#         ])
# 
#     def load_pairs(self):
#         """Load person-clothing pairs from txt file"""
#         pairs_file = os.path.join(self.data_dir, 'test_pairs.txt')
#         pairs = []
# 
#         with open(pairs_file, 'r') as f:
#             for line in f:
#                 line = line.strip()
#                 if line:
#                     person_img, cloth_img = line.split()
#                     pairs.append((person_img, cloth_img))
# 
#         return pairs
# 
#     def load_style_prompts(self):
#         """Load style-enhanced prompts from JSON"""
#         # Try to load style-enhanced prompts first, fallback to original
#         style_files_to_try = [
#             'kebap_with_style_news.json',       # ‚Üê YOUR CUSTOM STYLE JSON
#             'balanced_style_prompts.json',      # ‚Üê Style-enhanced balanced dataset
#             'style_prompts_training_only.json', # ‚Üê Style-enhanced training only
#             'style_prompts.json',               # ‚Üê Style-enhanced all prompts
#             'test_gpt4o.json'                   # ‚Üê Original prompts (fallback)
#         ]
# 
#         for filename in style_files_to_try:
#             gpt_file = os.path.join(self.data_dir, filename)
#             try:
#                 with open(gpt_file, 'r') as f:
#                     prompts = json.load(f)
#                 print(f"Loaded prompts from: {filename}")
#                 return prompts
#             except:
#                 continue
# 
#         print("Warning: No prompt files found")
#         return {}
# 
#     def format_prompt(self, image_id):
#         """Format prompt with existing attributes + style keyword"""
#         if image_id not in self.prompts:
#             return "A person wearing clothing"
# 
#         data = self.prompts[image_id]
#         person = data.get('person', {})
#         clothing = data.get('clothing', {})
#         style = data.get('style', None)  # ‚Üê Get style field
# 
#         prompt_parts = []
# 
#         # Person description
#         if person:
#             person_desc = []
#             if 'gender' in person:
#                 person_desc.append(f"{person['gender']}")
#             if 'body shape' in person:
#                 person_desc.append(f"with {person['body shape']} body shape")
#             if 'hair length' in person and 'hair style' in person:
#                 person_desc.append(f"with {person['hair length']} {person['hair style']} hair")
#             if 'pose' in person:
#                 person_desc.append(f"in a {person['pose']} pose")
# 
#             if person_desc:
#                 prompt_parts.append("A " + " ".join(person_desc))
# 
#         # Clothing description
#         if clothing:
#             clothing_desc = []
#             if 'upper cloth category' in clothing:
#                 clothing_desc.append(f"wearing a {clothing['upper cloth category']}")
#             if 'material' in clothing:
#                 clothing_desc.append(f"made of {clothing['material']}")
#             if 'neckline' in clothing:
#                 clothing_desc.append(f"with {clothing['neckline']} neckline")
#             if 'sleeve' in clothing:
#                 clothing_desc.append(f"and {clothing['sleeve']} sleeves")
#             if 'upper cloth length' in clothing:
#                 clothing_desc.append(f"in {clothing['upper cloth length']} length")
# 
#             if clothing_desc:
#                 prompt_parts.append(" ".join(clothing_desc))
# 
#         # Add style keyword at the end
#         if style:
#             prompt_parts.append(f"Style: {style}")
# 
#         if prompt_parts:
#             return ". ".join(prompt_parts) + "."
#         else:
#             return "A person wearing clothing."
# 
#     def load_image(self, image_path):
#         """Load and transform image"""
#         try:
#             image = Image.open(image_path).convert('RGB')
#             return self.transform(image)
#         except Exception as e:
#             print(f"Error loading image {image_path}: {e}")
#             dummy = Image.new('RGB', (self.image_size, self.image_size), color='black')
#             return self.transform(dummy)
# 
#     def load_mask(self, mask_path):
#         """Load and transform mask"""
#         try:
#             mask = Image.open(mask_path).convert('L')
#             mask = mask.resize((self.image_size, self.image_size))
#             mask_array = np.array(mask) / 255.0
#             mask_tensor = torch.from_numpy(mask_array).unsqueeze(0).float()
#             return mask_tensor
#         except Exception as e:
#             print(f"Error loading mask {mask_path}: {e}")
#             return torch.ones(1, self.image_size, self.image_size)
# 
#     def find_mask_path(self, person_id):
#         """Find mask file for person"""
#         mask_paths_to_try = [
#             # Test coarse'da _mask.png (CORRECT FORMAT)
#             os.path.join(self.data_dir, 'test_coarse', 'agnostic-mask', f'{person_id}_mask.png'),
#             # Test fine'da _mask.png
#             os.path.join(self.data_dir, f'test_{self.mode}', 'agnostic-mask', f'{person_id}_mask.png'),
#             # Fallback: normal format
#             os.path.join(self.data_dir, 'test_coarse', 'agnostic-mask', f'{person_id}.png'),
#             os.path.join(self.data_dir, f'test_{self.mode}', 'agnostic-mask', f'{person_id}.png'),
#         ]
# 
#         for mask_path in mask_paths_to_try:
#             if os.path.exists(mask_path):
#                 return mask_path
# 
#         print(f"No mask found for {person_id}")
#         return None
# 
#     def find_pose_path(self, person_id):
#         """Find pose file for person"""
#         pose_paths_to_try = [
#             # Dense pose
#             os.path.join(self.data_dir, f'test_{self.mode}', 'image-densepose', f'{person_id}.png'),
#             os.path.join(self.data_dir, f'test_{self.mode}', 'image-densepose', f'{person_id}.jpg'),
#             # OpenPose rendered
#             os.path.join(self.data_dir, f'test_{self.mode}', 'openpose_img', f'{person_id}_rendered.png'),
#         ]
# 
#         for pose_path in pose_paths_to_try:
#             if os.path.exists(pose_path):
#                 return pose_path
# 
#         return None
# 
#     def __getitem__(self, idx):
#         person_img_name, cloth_img_name = self.pairs[idx]
# 
#         person_id = os.path.splitext(person_img_name)[0]
#         cloth_id = os.path.splitext(cloth_img_name)[0]
# 
#         # Load images
#         person_img_path = os.path.join(self.data_dir, f'test_{self.mode}', 'image', person_img_name)
#         cloth_img_path = os.path.join(self.data_dir, f'test_{self.mode}', 'cloth', cloth_img_name)
# 
#         person_img = self.load_image(person_img_path)
#         cloth_img = self.load_image(cloth_img_path)
#         target_img = person_img.clone()
# 
#         # Find and load mask
#         mask_path = self.find_mask_path(person_id)
#         if mask_path:
#             mask = self.load_mask(mask_path)
#         else:
#             # Fallback dummy mask
#             mask = torch.ones(1, self.image_size, self.image_size) * 0.5
# 
#         # Find and load pose
#         pose_path = self.find_pose_path(person_id)
#         if pose_path:
#             pose_img = self.load_image(pose_path)
#         else:
#             pose_img = person_img.clone()
# 
#         # Generate style-enhanced prompt
#         prompt = self.format_prompt(person_id)
# 
#         # Get style information for debugging
#         style_info = self.prompts.get(person_id, {}).get('style', 'default')
# 
#         return {
#             'person': person_img,
#             'cloth': cloth_img,
#             'target': target_img,
#             'mask': mask,
#             'pose': pose_img,
#             'prompt': prompt,
#             'person_id': person_id,
#             'cloth_id': cloth_id,
#             'style': style_info  # ‚Üê Style info for monitoring
#         }
# 
#     def __len__(self):
#         return len(self.pairs)
# 
#     def get_style_distribution(self):
#         """Get distribution of styles in dataset for monitoring"""
#         style_counts = {}
#         for person_img_name, _ in self.pairs:
#             person_id = os.path.splitext(person_img_name)[0]  # Remove .jpg extension
#             style = self.prompts.get(person_id, {}).get('style', 'default')
#             style_counts[style] = style_counts.get(style, 0) + 1
#         return style_counts
# 
# if __name__ == "__main__":
#     # Test the dataset
#     dataset = PromptDresserStyleDataset(
#         data_dir="./DATA/zalando-hd-resized",  # ‚Üê YOUR DATASET PATH
#         mode="fine",
#         image_size=512
#     )
# 
#     print(f"Dataset size: {len(dataset)}")
# 
#     # Show style distribution
#     style_dist = dataset.get_style_distribution()
#     print(f"Style distribution: {style_dist}")
# 
#     # Test first few samples with style info
#     for i in range(min(3, len(dataset))):
#         sample = dataset[i]
#         print(f"\nSample {i}:")
#         print(f"  Person ID: {sample['person_id']}")
#         print(f"  Style: {sample['style']}")
#         print(f"  Prompt: {sample['prompt']}")
#         print(f"  Mask sum: {sample['mask'].sum():.2f}")
# 
#     print("\nStyle-enhanced dataset test completed!")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/drive/MyDrive/PromptDresser/configs/VITONHD_lora.yaml
# # PromptDresser Style-Enhanced LoRA Fine-tuning Configuration
# 
# model:
#   target: models.promptdresser.PromptDresserModel
#   params:
#     base_model_path: "./checkpoints/VITONHD/model/pytorch_model.bin"
#     #cloth_encoder_path: "./checkpoints/VITONHD/cloth_encoder.bin"  # ‚Üê EKLE
#     detach_cloth_encoder: false  # ‚Üê EKLE (mutual self-attention i√ßin)
# 
# lora:
#   enabled: true
#   rank: 16
#   alpha: 32
#   dropout: 0.1
#   target_modules:
#     - "to_q"
#     - "to_v"
#     - "to_k"
#     - "to_out.0"
#     # Cross-attention i√ßin kritik
#     - "attn1.to_q"  # ‚Üê EKLE (self-attention)
#     - "attn1.to_v"  # ‚Üê EKLE
#     - "attn1.to_k"  # ‚Üê EKLE
#     - "attn2.to_q"  # ‚Üê EKLE (cross-attention - style i√ßin kritik!)
#     - "attn2.to_v"  # ‚Üê EKLE
#     - "attn2.to_k"  # ‚Üê EKLE
# 
# training:
#   learning_rate: 2e-5  # ‚Üê ARTTIR (style learning i√ßin)
#   batch_size: 2        # ‚Üê ARTTIR (m√ºmk√ºnse)
#   max_epochs: 10       # ‚Üê ARTTIR (style √∂ƒürenmek zaman alƒ±r)
#   gradient_accumulation_steps: 4  # ‚Üê AZALT (batch_size arttƒ±)
#   save_steps: 50       # ‚Üê AZALT (daha sƒ±k save)
#   logging_steps: 10    # ‚Üê AZALT (daha sƒ±k log)
#   warmup_steps: 100    # ‚Üê ARTTIR
#   max_grad_norm: 1.0   # ‚Üê EKLE (gradient clipping)
# 
# data:
#   train_data_dir: "./DATA/zalando-hd-resized"
#   validation_split: 0.1
#   image_size: 512
#   # Style-specific settings
#   max_train_samples: 100  # ‚Üê EKLE (testing i√ßin)
#   style_focus: true       # ‚Üê EKLE (style training flag)
# 
# optimizer:
#   type: "AdamW"
#   weight_decay: 0.01
#   beta1: 0.9
#   beta2: 0.95  # ‚Üê DEƒûI≈ûTIR (style learning i√ßin daha iyi)
#   eps: 1e-6    # ‚Üê EKLE
# 
# scheduler:
#   type: "cosine"
#   min_lr: 1e-6
#   warmup_ratio: 0.1  # ‚Üê EKLE
# 
# # Style-specific configuration
# style_training:
#   enabled: true
#   supported_styles: ["summer", "winter","sport"]  # ‚Üê EKLE
#   style_weight: 1.0  # ‚Üê Style loss weight
# 
# # Logging and monitoring
# logging:
#   wandb:
#     enabled: false  # ‚Üê Set true if you want W&B logging
#     project: "promptdresser-style"
#   tensorboard:
#     enabled: true
#     log_dir: "./logs/style_lora"

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/drive/MyDrive/PromptDresser/configs/VITONHD_lora.yaml
# # PromptDresser LoRA Fine-tuning Configuration
# model:
#   target: models.promptdresser.PromptDresserModel
#   params:
#     base_model_path: "./checkpoints/VITONHD/model/pytorch_model.bin"
# 
# lora:
#   enabled: true
#   rank: 16
#   alpha: 32
#   dropout: 0.1
#   target_modules:
#     - "to_q"
#     - "to_v"
#     - "to_k"
#     - "to_out.0"
#     - "proj_in"
#     - "proj_out"
# 
# training:
#   learning_rate: 0.0001  # String deƒüil, number olarak
#   batch_size: 1
#   max_epochs: 5
#   gradient_accumulation_steps: 8
#   save_steps: 100
#   logging_steps: 50
#   warmup_steps: 50
# 
# data:
#   train_data_dir: "./DATA/zalando-hd-resized"
#   validation_split: 0.1
#   image_size: 512
# 
# optimizer:
#   type: "AdamW"
#   weight_decay: 0.01
#   beta1: 0.9
#   beta2: 0.999
# 
# scheduler:
#   type: "cosine"
#   min_lr: 0.000001  # Bu da number olarak

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/drive/MyDrive/PromptDresser/train_style_lora.py
# #!/usr/bin/env python3
# """
# STYLE-ENHANCED LORA TRAINING FOR PROMPTDRESSER
# Summer/Winter Style Fine-tuning with LoRA
# """
# 
# import torch
# import torch.nn as nn
# from torch.utils.data import DataLoader
# import argparse
# import yaml
# from pathlib import Path
# import logging
# from tqdm import tqdm
# import os
# import sys
# from diffusers import AutoencoderKL, DDPMScheduler
# from transformers import CLIPTextModel, CLIPTokenizer, CLIPTextModelWithProjection  # ‚Üê FIXED TYPO
# 
# # Add the dataset module
# sys.path.append('/content/drive/MyDrive/PromptDresser')
# from promptdresser_style_dataset import PromptDresserStyleDataset  # ‚Üê UPDATED
# 
# from promptdresser.models.unet import UNet2DConditionModel
# from promptdresser.models.cloth_encoder import ClothEncoder
# from promptdresser.models.mutual_self_attention import ReferenceAttentionControl
# from promptdresser.utils import load_file
# 
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)
# 
# class LoRALinear(nn.Module):
#     """LoRA-enhanced Linear layer optimized for style learning"""
#     def __init__(self, original_linear, rank=8, alpha=16):  # ‚Üê INCREASED rank for style
#         super().__init__()
#         self.original_linear = original_linear
#         self.rank = rank
#         self.alpha = alpha
#         self.scaling = alpha / rank
# 
#         device = original_linear.weight.device
#         dtype = original_linear.weight.dtype
# 
#         # LoRA parameters with improved initialization for style learning
#         self.lora_A = nn.Parameter(torch.zeros(original_linear.in_features, rank, device=device, dtype=dtype))
#         self.lora_B = nn.Parameter(torch.zeros(rank, original_linear.out_features, device=device, dtype=dtype))
# 
#         # Better initialization for style adaptation
#         nn.init.normal_(self.lora_A, mean=0.0, std=0.01)  # Slightly higher std
#         nn.init.zeros_(self.lora_B)
# 
#     def forward(self, x, scale=1.0, **kwargs):
#         result = self.original_linear(x)
# 
#         if self.training or scale != 0:
#             x_clamped = torch.clamp(x, min=-10, max=10)
#             lora_output = (x_clamped @ self.lora_A @ self.lora_B) * self.scaling * scale
#             result = result + lora_output
# 
#         return result
# 
# def replace_linear_with_lora(model, target_modules, rank=8, alpha=16):
#     """Replace linear layers with LoRA versions for style learning"""
#     lora_layers = {}
# 
#     for name, module in list(model.named_modules()):
#         if isinstance(module, nn.Linear) and any(target in name for target in target_modules):
#             *parent_names, attr_name = name.split('.')
#             parent = model
#             for pname in parent_names:
#                 parent = getattr(parent, pname)
# 
#             lora_layer = LoRALinear(module, rank=rank, alpha=alpha)
#             setattr(parent, attr_name, lora_layer)
#             lora_layers[name] = lora_layer
# 
#     return lora_layers
# 
# def create_time_ids(batch_size, device, dtype):
#     """Create time_ids for SDXL"""
#     original_size = (1024, 768)
#     crops_coords_top_left = (0, 0)
#     target_size = (1024, 768)
# 
#     time_ids = torch.tensor([
#         original_size[0], original_size[1],
#         crops_coords_top_left[0], crops_coords_top_left[1],
#         target_size[0], target_size[1]
#     ]).unsqueeze(0).float()
# 
#     return time_ids.repeat(batch_size, 1).to(device, dtype=dtype)
# 
# def validate_batch(batch):
#     """Validate batch data"""
#     for key, value in batch.items():
#         if torch.is_tensor(value):
#             if torch.isnan(value).any():
#                 logger.warning(f"NaN detected in batch['{key}']")
#                 return False
#             if torch.isinf(value).any():
#                 logger.warning(f"Inf detected in batch['{key}']")
#                 return False
#             if value.abs().max() > 100:
#                 logger.warning(f"Extreme values in batch['{key}']")
#                 return False
#     return True
# 
# def log_style_statistics(dataloader):
#     """Log style distribution for monitoring"""
#     style_counts = {}
#     for batch in dataloader:
#         for style in batch['style']:
#             style_counts[style] = style_counts.get(style, 0) + 1
# 
#     logger.info("Style distribution in training:")
#     for style, count in style_counts.items():
#         logger.info(f"  {style}: {count} samples")
# 
#     return style_counts
# 
# def main():
#     parser = argparse.ArgumentParser(description="Style-Enhanced LoRA Training for PromptDresser")
# 
# # ‚öôÔ∏è Config dosyasƒ±
#     parser.add_argument('--config', type=str, required=True, help="Config file path")
# 
# # üìÅ Veri yolu ve √ßƒ±kƒ±≈ü klas√∂r√º
#     parser.add_argument('--data_dir', type=str, default='./DATA/zalando-hd-resized', help="Custom dataset directory")
#     parser.add_argument('--output_dir', type=str, default='./checkpoints/style_lora_trainingg', help="LoRA checkpoints output")
# 
# # üîÅ Eƒüitim parametreleri
#     parser.add_argument('--num_train_epochs', type=int, default=5, help="Epochs for style learning")
#     parser.add_argument('--batch_size', type=int, default=1, help="Low batch size for VRAM optimization")
#     parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help="Avoid memory-heavy accumulation")
#     parser.add_argument('--learning_rate', type=float, default=2e-5, help="Higher LR for style adaptation")
#     parser.add_argument('--max_grad_norm', type=float, default=0.5, help="Prevent gradient explosion")
#     parser.add_argument('--max_train_samples', type=int, default=20, help="Limit for VRAM and faster training")
# 
#     # üß† LoRA Ayarlarƒ±
#     parser.add_argument('--lora_rank', type=int, default=8, help="LoRA rank for style fine-tuning")
#     parser.add_argument('--lora_alpha', type=int, default=16, help="LoRA alpha scale")
# 
#     # üìâ Log ve checkpoint ayarlarƒ±
#     parser.add_argument('--save_steps', type=int, default=100)
#     parser.add_argument('--logging_steps', type=int, default=10)
# 
#     # üßÆ Mixed precision (isteƒüe baƒülƒ±)
#     parser.add_argument('--mixed_precision', type=str, default='fp16', choices=["no", "fp16", "bf16"], help="Use mixed precision to reduce memory usage")
#     args = parser.parse_args()
# 
#     with open(args.config, 'r') as f:
#         config = yaml.safe_load(f)
# 
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#     weight_dtype = torch.float32
# 
#     os.makedirs(args.output_dir, exist_ok=True)
# 
#     # Load models
#     logger.info("Loading PromptDresser models...")
#     init_model_path = "./pretrained_models/stable-diffusion-xl-1.0-inpainting-0.1"
#     init_vae_path = "./pretrained_models/sdxl-vae-fp16-fix"
#     init_cloth_encoder_path = "./pretrained_models/stable-diffusion-xl-base-1.0"
# 
#     noise_scheduler = DDPMScheduler.from_pretrained(init_model_path, subfolder="scheduler")
#     tokenizer = CLIPTokenizer.from_pretrained(init_model_path, subfolder="tokenizer")
#     text_encoder = CLIPTextModel.from_pretrained(init_model_path, subfolder="text_encoder")
#     tokenizer_2 = CLIPTokenizer.from_pretrained(init_model_path, subfolder="tokenizer_2")  # Fix typo
#     text_encoder_2 = CLIPTextModelWithProjection.from_pretrained(init_model_path, subfolder="text_encoder_2")
#     vae = AutoencoderKL.from_pretrained(init_vae_path)
#     unet = UNet2DConditionModel.from_pretrained(init_model_path, subfolder="unet")
#     cloth_encoder = ClothEncoder.from_pretrained(init_cloth_encoder_path, subfolder="unet")
# 
#     # Load PromptDresser pretrained weights
#     if os.path.exists(config['model']['params']['base_model_path']):
#         unet.load_state_dict(load_file(config['model']['params']['base_model_path']))
#         logger.info("PromptDresser UNet weights loaded")
# 
#     if config['model']['params'].get('cloth_encoder_path'):
#         cloth_encoder.load_state_dict(load_file(config['model']['params']['cloth_encoder_path']), strict=False)
#         logger.info("Cloth encoder weights loaded")
# 
#     # Move to device
#     unet.to(device, dtype=weight_dtype)
#     vae.to(device, dtype=weight_dtype)
#     text_encoder.to(device, dtype=weight_dtype)
#     text_encoder_2.to(device, dtype=weight_dtype)
#     cloth_encoder.to(device, dtype=weight_dtype)
# 
#     # Freeze base models
#     unet.requires_grad_(False)
#     vae.requires_grad_(False)
#     text_encoder.requires_grad_(False)
#     text_encoder_2.requires_grad_(False)
#     cloth_encoder.requires_grad_(False)
# 
#     # Setup mutual self-attention
#     if not config.get("detach_cloth_encoder", False):
#         reference_control_writer = ReferenceAttentionControl(
#             cloth_encoder,
#             do_classifier_free_guidance=False,
#             mode="write",
#             fusion_blocks="midup" if os.environ.get("MIDUP_FUSION_BLOCK", False) else "full",
#             batch_size=args.batch_size,
#             is_train=True,
#         )
#         reference_control_reader = ReferenceAttentionControl(
#             unet,
#             do_classifier_free_guidance=False,
#             mode="read",
#             fusion_blocks="midup" if os.environ.get("MIDUP_FUSION_BLOCK", False) else "full",
#             batch_size=args.batch_size,
#             is_train=True,
#         )
# 
#     # Setup LoRA with higher rank for style learning
#     lora_rank = args.lora_rank
#     lora_alpha = args.lora_alpha
# 
#     lora_layers = replace_linear_with_lora(
#         unet,
#         target_modules=config['lora']['target_modules'],
#         rank=lora_rank,
#         alpha=lora_alpha
#     )
# 
#     # Collect LoRA parameters
#     lora_params = []
#     for name, lora_layer in lora_layers.items():
#         lora_layer.lora_A = lora_layer.lora_A.to(device, dtype=weight_dtype)
#         lora_layer.lora_B = lora_layer.lora_B.to(device, dtype=weight_dtype)
#         lora_params.extend([lora_layer.lora_A, lora_layer.lora_B])
# 
#     trainable_params = sum(p.numel() for p in lora_params)
#     logger.info(f"Style LoRA setup: {trainable_params:,} trainable parameters")
#     logger.info(f"Using rank={lora_rank}, alpha={lora_alpha} for style learning")
# 
#     # Setup STYLE dataset  ‚Üê KEY CHANGE
#     train_dataset = PromptDresserStyleDataset(
#         data_dir=args.data_dir,
#         mode='fine',
#         image_size=512
#     )
# 
#     # Log style distribution
#     style_dist = train_dataset.get_style_distribution()
#     logger.info(f"Style distribution: {style_dist}")
# 
#     if args.max_train_samples is not None:
#         train_dataset.pairs = train_dataset.pairs[:args.max_train_samples]
#         logger.info(f"Limited dataset to {args.max_train_samples} samples for testing")
# 
#     logger.info(f"Style-enhanced dataset size: {len(train_dataset)}")
# 
#     train_dataloader = DataLoader(
#         train_dataset,
#         batch_size=args.batch_size,
#         shuffle=True,
#         num_workers=0,
#         pin_memory=False,
#     )
# 
#     # Log training style distribution
#     # style_counts = log_style_statistics(train_dataloader)
# 
#     # Optimizer with settings optimized for style learning
#     optimizer = torch.optim.AdamW(
#         lora_params,
#         lr=args.learning_rate,
#         betas=(0.9, 0.95),  # Better for style adaptation
#         weight_decay=0.01,  # Small weight decay for regularization
#         eps=1e-6,
#     )
# 
#     # Training mode
#     unet.train()
# 
#     # Training loop
#     global_step = 0
#     first_epoch = 0
#     nan_count = 0
#     max_nan_count = 20
# 
#     logger.info("***** Running Style-Enhanced LoRA Training *****")
#     logger.info(f"  Num examples = {len(train_dataset)}")
#     logger.info(f"  Num Epochs = {args.num_train_epochs}")
#     logger.info(f"  Batch size = {args.batch_size}")
#     logger.info(f"  Style learning focus = Summer/Winter/Sporty")
# 
#     progress_bar = tqdm(
#         range(global_step, len(train_dataloader) * args.num_train_epochs),
#         desc="Style Training",
#     )
# 
#     for epoch in range(first_epoch, args.num_train_epochs):
#         epoch_loss = 0
#         valid_steps = 0
#         style_step_counts = {}
# 
#         for step, batch in enumerate(train_dataloader):
#             # Validate batch
#             if not validate_batch(batch):
#                 logger.warning(f"Invalid batch at step {step}, skipping...")
#                 continue
# 
#             # Track style distribution in training
#             for style in batch['style']:
#                 style_step_counts[style] = style_step_counts.get(style, 0) + 1
# 
#             # Track style distribution in training (optional)
#             batch_styles = batch.get('style', [])
# 
#             # Move batch to device
#             person_img = batch['person'].to(device, dtype=weight_dtype)
#             mask = batch['mask'].to(device, dtype=weight_dtype)
#             pose = batch['pose'].to(device, dtype=weight_dtype)
#             cloth = batch['cloth'].to(device, dtype=weight_dtype)
#             target_img = batch['target'].to(device, dtype=weight_dtype)
#             prompts = batch['prompt']  # ‚Üê These are now STYLE-ENHANCED prompts!  # ‚Üê Style-enhanced prompts
# 
#             batch_size = person_img.shape[0]
# 
#             # Encode images to latents
#             with torch.no_grad():
#                 person_latents = vae.encode(person_img).latent_dist.sample() * vae.config.scaling_factor
#                 target_latents = vae.encode(target_img).latent_dist.sample() * vae.config.scaling_factor
#                 cloth_latents = vae.encode(cloth).latent_dist.sample() * vae.config.scaling_factor
# 
#                 if torch.isnan(person_latents).any() or torch.isnan(target_latents).any() or torch.isnan(cloth_latents).any():
#                     logger.warning(f"NaN in latents at step {step}, skipping...")
#                     continue
# 
#                 # Downsample mask
#                 mask_latents = torch.nn.functional.interpolate(
#                     mask, size=person_latents.shape[-2:], mode='nearest'
#                 )
# 
#                 # Encode style-enhanced prompts
#                 text_inputs = tokenizer(
#                     prompts,
#                     padding="max_length",
#                     max_length=77,
#                     truncation=True,
#                     return_tensors="pt"
#                 ).to(device)
#                 text_inputs_2 = tokenizer_2(
#                     prompts,
#                     padding="max_length",
#                     max_length=77,
#                     truncation=True,
#                     return_tensors="pt"
#                 ).to(device)
# 
#                 text_embeddings = text_encoder(text_inputs.input_ids)[0]
#                 text_embeddings_2_output = text_encoder_2(text_inputs_2.input_ids)
#                 text_embeddings_2 = text_embeddings_2_output.last_hidden_state
#                 pooled_text_embeddings = text_embeddings_2_output.text_embeds
# 
#                 encoder_hidden_states = torch.cat([text_embeddings, text_embeddings_2], dim=-1)
# 
#                 # Cloth encoder features
#                 if not config.get("detach_cloth_encoder", False):
#                     time_ids_cloth = create_time_ids(batch_size, device, weight_dtype)
#                     added_cond_kwargs_cloth = {
#                         'text_embeds': pooled_text_embeddings,
#                         'time_ids': time_ids_cloth
#                     }
# 
#                     _ = cloth_encoder(
#                         cloth_latents,
#                         timestep=torch.zeros(batch_size, device=device, dtype=torch.long),
#                         encoder_hidden_states=encoder_hidden_states,  # ‚Üê Style-enhanced embeddings
#                         added_cond_kwargs=added_cond_kwargs_cloth,
#                         return_dict=False,
#                     )[0]
# 
#             # Sample noise and timesteps
#             noise = torch.randn_like(target_latents)
#             timesteps = torch.randint(
#                 0, noise_scheduler.config.num_train_timesteps, (batch_size,), device=device
#             ).long()
# 
#             # Add noise
#             noisy_latents = noise_scheduler.add_noise(target_latents, noise, timesteps)
# 
#             # UNet input
#             unet_input = torch.cat([noisy_latents, mask_latents, person_latents], dim=1)
# 
#             # Time IDs
#             time_ids = create_time_ids(batch_size, device, weight_dtype)
#             added_cond_kwargs = {
#                 'text_embeds': pooled_text_embeddings,
#                 'time_ids': time_ids
#             }
# 
#             # Forward pass - Style-enhanced prediction
#             try:
#                 model_pred = unet(
#                     sample=unet_input,
#                     timestep=timesteps,
#                     encoder_hidden_states=encoder_hidden_states,  # ‚Üê Style-enhanced
#                     added_cond_kwargs=added_cond_kwargs,
#                     return_dict=True
#                 ).sample
# 
#                 if torch.isnan(model_pred).any():
#                     logger.warning(f"NaN in model predictions at step {step}")
#                     nan_count += 1
#                     if nan_count > max_nan_count:
#                         logger.error(f"Too many NaN predictions, stopping training")
#                         return
#                     continue
# 
#                 # Style-focused loss calculation
#                 model_pred_clamped = torch.clamp(model_pred, min=-10, max=10)
#                 noise_clamped = torch.clamp(noise, min=-10, max=10)
#                 loss = nn.functional.mse_loss(model_pred_clamped.float(), noise_clamped.float(), reduction="mean")
# 
#                 if torch.isnan(loss):
#                     logger.warning(f"NaN loss at step {step}")
#                     nan_count += 1
#                     continue
# 
#                 loss = loss / args.gradient_accumulation_steps
# 
#             except Exception as e:
#                 logger.error(f"Error in forward pass at step {step}: {e}")
#                 continue
# 
#             epoch_loss += loss.detach().item()
#             valid_steps += 1
# 
#             # Backward
#             loss.backward()
# 
#             # Gradient accumulation
#             if (step + 1) % args.gradient_accumulation_steps == 0:
#                 # Handle NaN gradients
#                 for lora_layer in lora_layers.values():
#                     if lora_layer.lora_A.grad is not None and torch.isnan(lora_layer.lora_A.grad).any():
#                         logger.warning("NaN gradient in lora_A, zeroing...")
#                         lora_layer.lora_A.grad.zero_()
#                     if lora_layer.lora_B.grad is not None and torch.isnan(lora_layer.lora_B.grad).any():
#                         logger.warning("NaN gradient in lora_B, zeroing...")
#                         lora_layer.lora_B.grad.zero_()
# 
#                 # Gradient clipping
#                 torch.nn.utils.clip_grad_norm_(lora_params, args.max_grad_norm)
# 
#                 # Optimizer step
#                 optimizer.step()
#                 optimizer.zero_grad()
# 
#                 global_step += 1
#                 progress_bar.update(1)
# 
#                 # Enhanced logging with style info
#                 if global_step % args.logging_steps == 0 and valid_steps > 0:
#                     avg_loss = epoch_loss / valid_steps * args.gradient_accumulation_steps
#                     progress_bar.set_postfix({
#                         'loss': f'{avg_loss:.4f}',
#                         'lr': f'{args.learning_rate:.2e}',
#                         'styles': len(style_step_counts),
#                         'nan': nan_count
#                     })
#                     logger.info(f"Step {global_step}: loss={avg_loss:.4f}, styles_seen={list(style_step_counts.keys())}")
# 
#                 # Save checkpoint with style info
#                 if global_step % args.save_steps == 0:
#                     checkpoint_dir = Path(args.output_dir) / f"style-checkpoint-{global_step}"
#                     checkpoint_dir.mkdir(parents=True, exist_ok=True)
# 
#                     # Save LoRA weights
#                     lora_state_dict = {}
#                     for name, lora_layer in lora_layers.items():
#                         lora_state_dict[f"{name}.lora_A"] = lora_layer.lora_A.data.cpu()
#                         lora_state_dict[f"{name}.lora_B"] = lora_layer.lora_B.data.cpu()
# 
#                     checkpoint = {
#                         'lora_weights': lora_state_dict,
#                         'optimizer_state_dict': optimizer.state_dict(),
#                         'global_step': global_step,
#                         'epoch': epoch,
#                         'config': config,
#                         'style_distribution': style_dist,
#                         'lora_rank': lora_rank,
#                         'lora_alpha': lora_alpha,
#                         'training_styles': ['summer', 'winter']
#                     }
# 
#                     torch.save(checkpoint, checkpoint_dir / "style_checkpoint.pt")
#                     logger.info(f"Style checkpoint saved to {checkpoint_dir}")
# 
#         # End of epoch logging
#         if valid_steps > 0:
#             avg_epoch_loss = epoch_loss / valid_steps
#             logger.info(f"Epoch {epoch} completed: avg_loss={avg_epoch_loss:.4f}, styles_trained={style_step_counts}")
# 
#     # Save final style-enhanced model
#     final_dir = Path(args.output_dir) / "final_style_lora"
#     final_dir.mkdir(parents=True, exist_ok=True)
# 
#     # Save LoRA weights
#     lora_state_dict = {}
#     for name, lora_layer in lora_layers.items():
#         lora_state_dict[f"{name}.lora_A"] = lora_layer.lora_A.data.cpu()
#         lora_state_dict[f"{name}.lora_B"] = lora_layer.lora_B.data.cpu()
# 
#     final_checkpoint = {
#         'lora_weights': lora_state_dict,
#         'config': config,
#         'style_capabilities': ['summer', 'winter'],
#         'training_samples': len(train_dataset),
#         'lora_rank': lora_rank,
#         'lora_alpha': lora_alpha,
#         'style_distribution': style_dist
#     }
# 
#     torch.save(final_checkpoint, final_dir / "style_lora_weights.pt")
# 
#     # Save config
#     with open(final_dir / "style_config.yaml", 'w') as f:
#         yaml.dump(config, f)
# 
#     # Save style info
#     with open(final_dir / "style_info.txt", 'w') as f:
#         f.write("STYLE-ENHANCED PROMPTDRESSER LORA\n")
#         f.write("================================\n\n")
#         f.write(f"Training completed with {len(train_dataset)} samples\n")
#         f.write(f"Style distribution: {style_dist}\n")
#         f.write(f"LoRA rank: {lora_rank}, alpha: {lora_alpha}\n")
#         f.write(f"Total NaN encounters: {nan_count}\n\n")
#         f.write("Supported styles:\n")
#         f.write("- summer_casual: relaxed, untucked, casual summer style\n")
#         f.write("- winter_formal: fitted, tucked, professional winter style\n")
#         f.write("- sporty_athletic: comfortable, casual sporty style\n")
# 
#     logger.info(f"Style-enhanced LoRA training completed!")
#     logger.info(f"Final model saved to {final_dir}")
#     logger.info(f"Model supports: {list(style_dist.keys())}")
# 
# if __name__ == "__main__":
#     main()

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/PromptDresser

# √ñnce dataset'i doƒüru yere koy
!cp promptdresser_dataset_correct.py promptdresser/data/dataset.py

# Training'i ba≈ülat (k√º√ß√ºk bir subset ile test et √∂nce)
!python train_lora_WITH_DATASET.py \
    --config configs/lora_training_config.yaml \
    --data_dir ./DATA/zalando-hd-resized \
    --batch_size 1 \
    --gradient_accumulation_steps 4 \
    --num_train_epochs 5 \
    --max_train_samples 100 \
    --learning_rate 1e-4 \
    --save_steps 50 \
    --logging_steps 10

!ls -la /content/drive/MyDrive/PromptDresser/promptdresser/

# Colab'da √ßalƒ±≈ütƒ±r - UNet signature test
from promptdresser.models.unet import UNet2DConditionModel
import inspect

# UNet'i y√ºkle
unet = UNet2DConditionModel.from_pretrained("./pretrained_models/stable-diffusion-xl-1.0-inpainting-0.1", subfolder="unet")

# Forward method signature'ƒ±nƒ± g√∂ster
sig = inspect.signature(unet.forward)
print("UNet forward signature:")
print(sig)

# Parameters
for name, param in sig.parameters.items():
    print(f"  {name}: {param}")

# Dosyayƒ± kontrol et

!grep -n "pose_encoder_input" train_lora_WORKING.py

!pip show diffusers transformers peft torch

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/PromptDresser
!python train_lora_PIPELINE_STYLE.py --config configs/VITONHD_lora.yaml

import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/PromptDresser

!CUDA_VISIBLE_DEVICES=0 python inference.py \
  --config_p "./configs/VITONHD_lora.yaml" \
  --pretrained_unet_path "./checkpoints/VITONHD/model/pytorch_model.bin" \
  --lora_weights_path "./checkpoints/style_lora_trainingg/final_style_lora" \
  --save_name "last3" \
  --skip_unpaired \
  --s_idx 0 --e_idx 1

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/PromptDresser

import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"


!python train_style_lora.py \
  --config configs/VITONHD_lora.yaml \
  --output_dir ./checkpoints/style_lora_trainingg

# GPU'yu resetle
import torch
torch.cuda.empty_cache()

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/PromptDresser

# ULTRA FINAL version
!python train_lora_ULTRA_FINAL.py --config configs/VITONHD_lora.yaml

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/PromptDresser

# Basit test - tek satƒ±rda
!python -c "from promptdresser_dataset import PromptDresserDataset; dataset = PromptDresserDataset('./DATA/zalando-hd-resized', 'coarse'); print(f'Dataset: {len(dataset)} samples')"

# PromptDresser UNet'inin forward signature'ƒ±nƒ± g√∂relim
!head -50 /content/drive/MyDrive/PromptDresser/promptdresser/models/unet.py

# Ya da UNet class'ƒ±nƒ±n forward method'unu bul
!grep -n "def forward" /content/drive/MyDrive/PromptDresser/promptdresser/models/unet.py

# Dependencies'leri uyumlu versiyonlara getir
!pip install huggingface-hub==0.24.6 --force-reinstall
!pip install diffusers==0.25.0 --force-reinstall
!pip install transformers==4.26.1 --force-reinstall

# Runtime restart gerekebilir

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/PromptDresser

# Dataset test
!python promptdresser_dataset_correct.py

# Custom modules import test - FIXED
!python -c "from promptdresser.models.unet import UNet2DConditionModel; print('Custom modules import SUCCESS!')"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/PromptDresser

# Dataset'i doƒüru import eden training script'inde d√ºzelt
!sed -i 's/from promptdresser_dataset_fixed/from promptdresser_dataset_correct/g' train_lora_final_complete.py

# Full PromptDresser LoRA training ba≈ülat
!python train_lora_final_complete.py --config configs/VITONHD_lora.yaml

!pip show diffusers | grep Version

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/PromptDresser

# Custom modules training script'inde dataset'i g√ºncelle
!sed -i 's/promptdresser_dataset_fixed/promptdresser_dataset_correct/g' train_lora_final_complete.py

# Training ba≈ülat
!python train_lora_final_complete.py --config configs/VITONHD_lora.yaml

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/PromptDresser

# Correct mask format ile test
!python promptdresser_dataset_correct.py

# Mevcut klas√∂r yapƒ±sƒ±nƒ± kontrol et
!ls -la /content/drive/MyDrive/PromptDresser/DATA/zalando-hd-resized/

# Test klas√∂rlerini kontrol et
!ls -la /content/drive/MyDrive/PromptDresser/DATA/zalando-hd-resized/test_*/

# Agnostic mask klas√∂rleri var mƒ±?
!find /content/drive/MyDrive/PromptDresser/DATA/zalando-hd-resized/ -name "*agnostic*" -type d

# Mask dosyalarƒ±nƒ± ara
!find /content/drive/MyDrive/PromptDresser/DATA/zalando-hd-resized/ -name "*mask*" | head -10

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/drive/MyDrive/PromptDresser/promptdresser_style_dataset.py
# import os
# import json
# import numpy as np
# from PIL import Image
# import torch
# from torch.utils.data import Dataset
# from torchvision import transforms
# from torchvision.transforms import InterpolationMode as IM
# 
# class PromptDresserStyleDataset(Dataset):
#     """
#     PromptDresser Dataset with Style Support (Summer/Winter/Casual/Sport)
#     VRAM-dostu g√ºncellemeler:
#       - RGB normalize 3-kanal
#       - target clone kaldƒ±rƒ±ldƒ±
#       - pose opsiyonel (default: kapalƒ±)
#     """
#     def __init__(self, data_dir, mode='fine', image_size=512, return_pose=False):
#         self.data_dir = data_dir
#         self.mode = mode
#         self.image_size = image_size
#         self.return_pose = return_pose  # GPU‚Äôya gereksiz y√ºk bindirmemek i√ßin
# 
#         self.pairs = self.load_pairs()
#         self.prompts = self.load_style_prompts()
# 
#         self.transform = transforms.Compose([
#             transforms.Resize((image_size, image_size), interpolation=IM.BILINEAR),
#             transforms.ToTensor(),
#             transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),
#         ])
# 
#     def load_pairs(self):
#         """Load person-clothing pairs from txt file (test_pairs.txt)"""
#         pairs_file = os.path.join(self.data_dir, 'test_pairs.txt')
#         pairs = []
#         if not os.path.exists(pairs_file):
#             print(f"[WARN] pairs file not found: {pairs_file}")
#             return pairs
# 
#         with open(pairs_file, 'r') as f:
#             for line in f:
#                 line = line.strip()
#                 if not line:
#                     continue
#                 toks = line.split()
#                 if len(toks) < 2:
#                     continue
#                 person_img, cloth_img = toks[0], toks[1]
#                 pairs.append((person_img, cloth_img))
#         return pairs
# 
#     def load_style_prompts(self):
#         """Load style-enhanced prompts from JSON (tries a few filenames)"""
#         candidates = [
#             'kebap_with_style_news.json',
#             'balanced_style_prompts.json',
#             'style_prompts_training_only.json',
#             'style_prompts.json',
#             'test_gpt4o.json'
#         ]
#         for fn in candidates:
#             p = os.path.join(self.data_dir, fn)
#             if os.path.exists(p):
#                 try:
#                     with open(p, 'r') as f:
#                         data = json.load(f)
#                     print(f"[INFO] Loaded prompts from: {fn}")
#                     return data
#                 except Exception as e:
#                     print(f"[WARN] Failed to load {fn}: {e}")
#         print("[WARN] No prompt files found.")
#         return {}
# 
#     def format_prompt(self, image_id):
#         """Format prompt with existing attributes + style keyword"""
#         if image_id not in self.prompts:
#             return "A person wearing clothing."
# 
#         data = self.prompts[image_id]
#         person = data.get('person', {})
#         clothing = data.get('clothing', {})
#         style = data.get('style', None)
# 
#         parts = []
# 
#         # Person
#         if person:
#             p_desc = []
#             g = person.get('gender')
#             if g: p_desc.append(str(g))
#             bs = person.get('body shape')
#             if bs: p_desc.append(f"with {bs} body shape")
#             hl = person.get('hair length')
#             hs = person.get('hair style')
#             if hl and hs: p_desc.append(f"with {hl} {hs} hair")
#             pose = person.get('pose')
#             if pose: p_desc.append(f"in a {pose} pose")
#             if p_desc:
#                 parts.append("A " + " ".join(p_desc))
# 
#         # Clothing
#         if clothing:
#             c_desc = []
#             cat = clothing.get('upper cloth category')
#             if cat: c_desc.append(f"wearing a {cat}")
#             mat = clothing.get('material')
#             if mat: c_desc.append(f"made of {mat}")
#             neck = clothing.get('neckline')
#             if neck: c_desc.append(f"with {neck} neckline")
#             sleeve = clothing.get('sleeve')
#             if sleeve: c_desc.append(f"and {sleeve} sleeves")
#             length = clothing.get('upper cloth length')
#             if length: c_desc.append(f"in {length} length")
#             if c_desc:
#                 parts.append(" ".join(c_desc))
# 
#         if style:
#             parts.append(f"Style: {style}")
# 
#         return (". ".join(parts) + ".") if parts else "A person wearing clothing."
# 
#     def load_image(self, path):
#         """Load and transform RGB image with safe fallback"""
#         try:
#             img = Image.open(path).convert('RGB')
#         except Exception as e:
#             print(f"[WARN] Error loading image {path}: {e}; using black dummy.")
#             img = Image.new('RGB', (self.image_size, self.image_size), color='black')
#         return self.transform(img)
# 
#     def load_mask(self, path):
#         """Load and transform mask as 1xHxW float tensor (0..1)"""
#         try:
#             m = Image.open(path).convert('L')
#             m = m.resize((self.image_size, self.image_size), resample=IM.NEAREST)
#             arr = np.array(m, dtype=np.float32) / 255.0
#             return torch.from_numpy(arr).unsqueeze(0)
#         except Exception as e:
#             print(f"[WARN] Error loading mask {path}: {e}; using 0.5 dummy.")
#             return torch.full((1, self.image_size, self.image_size), 0.5, dtype=torch.float32)
# 
#     def find_mask_path(self, person_id):
#         """Find mask file for person"""
#         tries = [
#             os.path.join(self.data_dir, 'test_coarse', 'agnostic-mask', f'{person_id}_mask.png'),
#             os.path.join(self.data_dir, f'test_{self.mode}', 'agnostic-mask', f'{person_id}_mask.png'),
#             os.path.join(self.data_dir, 'test_coarse', 'agnostic-mask', f'{person_id}.png'),
#             os.path.join(self.data_dir, f'test_{self.mode}', 'agnostic-mask', f'{person_id}.png'),
#         ]
#         for p in tries:
#             if os.path.exists(p):
#                 return p
#         # no print spam; return None silently
#         return None
# 
#     def find_pose_path(self, person_id):
#         """Find pose file for person (only if return_pose=True)"""
#         tries = [
#             os.path.join(self.data_dir, f'test_{self.mode}', 'image-densepose', f'{person_id}.png'),
#             os.path.join(self.data_dir, f'test_{self.mode}', 'image-densepose', f'{person_id}.jpg'),
#             os.path.join(self.data_dir, f'test_{self.mode}', 'openpose_img', f'{person_id}_rendered.png'),
#         ]
#         for p in tries:
#             if os.path.exists(p):
#                 return p
#         return None
# 
#     def __getitem__(self, idx):
#         person_img_name, cloth_img_name = self.pairs[idx]
#         person_id = os.path.splitext(person_img_name)[0]
#         cloth_id  = os.path.splitext(cloth_img_name)[0]
# 
#         # paths
#         person_path = os.path.join(self.data_dir, f'test_{self.mode}', 'image', person_img_name)
#         cloth_path  = os.path.join(self.data_dir, f'test_{self.mode}', 'cloth', cloth_img_name)
# 
#         # images
#         person_img = self.load_image(person_path)
#         cloth_img  = self.load_image(cloth_path)
# 
#         # hedef (aynƒ± g√∂r√ºnt√º; clone ile kopya yaratmƒ±yoruz)
#         target_img = person_img  # .clone() yok -> daha az RAM/VRAM
# 
#         # mask
#         mask_path = self.find_mask_path(person_id)
#         mask = self.load_mask(mask_path) if mask_path else torch.full(
#             (1, self.image_size, self.image_size), 0.5, dtype=torch.float32
#         )
# 
#         # pose (opsiyonel; default: yok)
#         if self.return_pose:
#             pose_path = self.find_pose_path(person_id)
#             pose_img = self.load_image(pose_path) if pose_path else person_img
#         else:
#             # 1xHxW dummy; eƒüitim kodu isterse CPU‚Äôda kalsƒ±n
#             pose_img = torch.zeros(1, self.image_size, self.image_size, dtype=torch.float32)
# 
#         # prompt
#         prompt = self.format_prompt(person_id)
#         style_info = self.prompts.get(person_id, {}).get('style', 'default')
# 
#         return {
#             'person': person_img,
#             'cloth': cloth_img,
#             'target': target_img,
#             'mask': mask,
#             'pose': pose_img,        # eƒüitimde GPU‚Äôya ta≈üƒ±mayƒ± kaldƒ±r
#             'prompt': prompt,
#             'person_id': person_id,
#             'cloth_id': cloth_id,
#             'style': style_info
#         }
# 
#     def __len__(self):
#         return len(self.pairs)
# 
#     def get_style_distribution(self):
#         """Style histogram (for monitoring)"""
#         counts = {}
#         for person_img_name, _ in self.pairs:
#             pid = os.path.splitext(person_img_name)[0]
#             style = self.prompts.get(pid, {}).get('style', 'default')
#             counts[style] = counts.setdefault(style, 0) + 1
#         return counts
# 
# if __name__ == "__main__":
#     ds = PromptDresserStyleDataset(
#         data_dir="./DATA/zalando-hd-resized",
#         mode="fine",
#         image_size=512,
#         return_pose=False
#     )
#     print(f"Dataset size: {len(ds)}")
#     print("Style dist:", ds.get_style_distribution())
#     for i in range(min(3, len(ds))):
#         s = ds[i]
#         print(f"[{i}] {s['person_id']}, style={s['style']}, mask_sum={float(s['mask'].sum()):.2f}")
#

# Commented out IPython magic to ensure Python compatibility.
# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-
# %%writefile /content/drive/MyDrive/PromptDresser/train_style_lora.py
# """
# STYLE-ENHANCED LORA TRAINING FOR PROMPTDRESSER (VRAM-OPTIMIZED)
# - Mixed precision (fp16/bf16)
# - Gradient checkpointing
# - xFormers (varsa) / attention slicing fallback
# - VAE tiling/slicing
# - Regex tabanlƒ± LoRA injection (UNet attention projeksiyonlarƒ±)
# - Cloth encoder'ƒ± ko≈üullu devre dƒ±≈üƒ± (VRAM kazanƒ±mƒ±)
# """
# 
# import os
# import re
# import sys
# import yaml
# import math
# import torch
# import torch.nn as nn
# from torch.utils.data import DataLoader
# import argparse
# import logging
# from pathlib import Path
# from tqdm import tqdm
# 
# from diffusers import AutoencoderKL, DDPMScheduler
# from transformers import CLIPTextModel, CLIPTokenizer, CLIPTextModelWithProjection
# 
# # PromptDresser repo yollarƒ±
# sys.path.append('/content/drive/MyDrive/PromptDresser')
# 
# from promptdresser_style_dataset import PromptDresserStyleDataset  # Senin dataset sƒ±nƒ±fƒ±n
# from promptdresser.models.unet import UNet2DConditionModel
# from promptdresser.models.cloth_encoder import ClothEncoder
# from promptdresser.models.mutual_self_attention import ReferenceAttentionControl
# from promptdresser.utils import load_file
# 
# # ---------------------------------------------------------------------
# # Logging
# # ---------------------------------------------------------------------
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger("style_lora")
# 
# # ---------------------------------------------------------------------
# # LoRA Katmanƒ±
# # ---------------------------------------------------------------------
# class LoRALinear(nn.Module):
#     """LoRA-enhanced Linear layer (A @ B d√º≈ü√ºk-rank ekleme)"""
#     def __init__(self, original_linear: nn.Linear, rank=8, alpha=16):
#         super().__init__()
#         self.original_linear = original_linear
#         self.rank = rank
#         self.alpha = alpha
#         self.scaling = alpha / max(1, rank)
# 
#         # LoRA parametreleri
#         device = original_linear.weight.device
#         dtype = original_linear.weight.dtype
#         self.lora_A = nn.Parameter(torch.zeros(original_linear.in_features, rank, device=device, dtype=dtype))
#         self.lora_B = nn.Parameter(torch.zeros(rank, original_linear.out_features, device=device, dtype=dtype))
# 
#         # Ba≈ülangƒ±√ß (stil adaptasyonu i√ßin hafif√ße artƒ±rƒ±lmƒ±≈ü std)
#         nn.init.normal_(self.lora_A, mean=0.0, std=0.01)
#         nn.init.zeros_(self.lora_B)
# 
#     def forward(self, x, scale: float = 1.0, **kwargs):
#         out = self.original_linear(x)
#         # Eƒüitimde veya inference'ta scale>0 ise LoRA katkƒ±sƒ±nƒ± ekle
#         if self.training or scale != 0:
#             x_c = torch.clamp(x, -10, 10)
#             out = out + (x_c @ self.lora_A @ self.lora_B) * self.scaling * scale
#         return out
# 
# def replace_linear_with_lora(model: nn.Module, target_patterns, rank=8, alpha=16):
#     """
#     named_modules() i√ßindeki Linear katmanlardan regex ile e≈üle≈üenleri LoRA ile deƒüi≈ütir.
#     target_patterns: regex listesi
#     """
#     compiled = [re.compile(p) for p in target_patterns]
#     lora_layers = {}
# 
#     for name, module in list(model.named_modules()):
#         if isinstance(module, nn.Linear) and any(p.search(name) for p in compiled):
#             *parents, attr = name.split(".")
#             parent = model
#             for p in parents:
#                 parent = getattr(parent, p)
#             lora_layer = LoRALinear(module, rank=rank, alpha=alpha)
#             setattr(parent, attr, lora_layer)
#             lora_layers[name] = lora_layer
# 
#     return lora_layers
# 
# # ---------------------------------------------------------------------
# # Yardƒ±mcƒ±lar
# # ---------------------------------------------------------------------
# def create_time_ids(batch_size, device, dtype):
#     """SDXL time_ids"""
#     original_size = (1024, 768)
#     crops_coords_top_left = (0, 0)
#     target_size = (1024, 768)
#     time_ids = torch.tensor([
#         original_size[0], original_size[1],
#         crops_coords_top_left[0], crops_coords_top_left[1],
#         target_size[0], target_size[1]
#     ], dtype=torch.float32).unsqueeze(0)
#     return time_ids.repeat(batch_size, 1).to(device=device, dtype=dtype)
# 
# def validate_batch(batch):
#     """Basit NaN/Inf kontrol√º"""
#     for k, v in batch.items():
#         if torch.is_tensor(v):
#             if torch.isnan(v).any() or torch.isinf(v).any():
#                 logger.warning(f"Bad tensor in batch['{k}']")
#                 return False
#     return True
# 
# def try_get_style_dist(ds: PromptDresserStyleDataset):
#     """Dataset'ten stil daƒüƒ±lƒ±mƒ±nƒ± topla."""
#     try:
#         return ds.get_style_distribution()
#     except Exception:
#         counts = {}
#         if hasattr(ds, "pairs"):
#             for p in ds.pairs:
#                 style = p.get("style", "unknown")
#                 counts[style] = counts.get(style, 0) + 1
#         return counts
# 
# # ---------------------------------------------------------------------
# # Arg√ºmanlar
# # ---------------------------------------------------------------------
# def parse_args():
#     ap = argparse.ArgumentParser(description="Style-Enhanced LoRA Training for PromptDresser (VRAM-optimized)")
#     # Config
#     ap.add_argument("--config", type=str, required=True, help="YAML config yolu")
#     # Veri ve √ßƒ±kƒ±≈ü
#     ap.add_argument("--data_dir", type=str, default="./DATA/zalando-hd-resized", help="Dataset klas√∂r√º")
#     ap.add_argument("--output_dir", type=str, default="./checkpoints/style_lora_training", help="Checkpoint klas√∂r√º")
#     # Eƒüitim
#     ap.add_argument("--num_train_epochs", type=int, default=3)
#     ap.add_argument("--batch_size", type=int, default=1)
#     ap.add_argument("--gradient_accumulation_steps", type=int, default=8)
#     ap.add_argument("--learning_rate", type=float, default=5e-5)
#     ap.add_argument("--max_grad_norm", type=float, default=0.5)
#     ap.add_argument("--max_train_samples", type=int, default=20)  # az-veri senaryosu
#     ap.add_argument("--image_size", type=int, default=384)        # 512 -> 384 VRAM i√ßin
#     # LoRA
#     ap.add_argument("--lora_rank", type=int, default=8)
#     ap.add_argument("--lora_alpha", type=int, default=16)
#     # Kayƒ±t/Log
#     ap.add_argument("--save_steps", type=int, default=100)
#     ap.add_argument("--logging_steps", type=int, default=10)
#     # Precision
#     ap.add_argument("--mixed_precision", type=str, default="fp16", choices=["no", "fp16", "bf16"])
#     # LoRA scale (inference'ta i≈üine yarar; eƒüitimde bilgi ama√ßlƒ±)
#     ap.add_argument("--lora_scale", type=float, default=1.0)
#     return ap.parse_args()
# 
# # ---------------------------------------------------------------------
# # Ana
# # ---------------------------------------------------------------------
# def main():
#     args = parse_args()
#     with open(args.config, "r") as f:
#         config = yaml.safe_load(f)
# 
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# 
#     # Precision se√ßimi
#     if args.mixed_precision == "bf16":
#         weight_dtype = torch.bfloat16
#     elif args.mixed_precision == "fp16":
#         weight_dtype = torch.float16
#     else:
#         weight_dtype = torch.float32
# 
#     os.makedirs(args.output_dir, exist_ok=True)
# 
#     # -----------------------------------------------------------------
#     # Modelleri y√ºkle
#     # -----------------------------------------------------------------
#     logger.info("Loading base models...")
#     init_model_path = "./pretrained_models/stable-diffusion-xl-1.0-inpainting-0.1"
#     init_vae_path   = "./pretrained_models/sdxl-vae-fp16-fix"
#     init_cloth_encoder_path = "./pretrained_models/stable-diffusion-xl-base-1.0"
# 
#     noise_scheduler = DDPMScheduler.from_pretrained(init_model_path, subfolder="scheduler")
# 
#     tokenizer   = CLIPTokenizer.from_pretrained(init_model_path, subfolder="tokenizer")
#     text_enc    = CLIPTextModel.from_pretrained(init_model_path, subfolder="text_encoder")
#     tokenizer_2 = CLIPTokenizer.from_pretrained(init_model_path, subfolder="tokenizer_2")
#     text_enc_2  = CLIPTextModelWithProjection.from_pretrained(init_model_path, subfolder="text_encoder_2")
# 
#     vae  = AutoencoderKL.from_pretrained(init_vae_path)
#     unet = UNet2DConditionModel.from_pretrained(init_model_path, subfolder="unet")
# 
#     # Cloth encoder opsiyonel (VRAM i√ßin kapatƒ±labilir)
#     detach_cloth = bool(config.get("detach_cloth_encoder", True))  # default: True (VRAM dostu)
#     cloth_encoder = None
#     if not detach_cloth:
#         cloth_encoder = ClothEncoder.from_pretrained(init_cloth_encoder_path, subfolder="unet")
# 
#     # PromptDresser √∂n-eƒüitimli UNet/cloth aƒüƒ±rlƒ±klarƒ±
#     if "model" in config and "params" in config["model"]:
#         base_unet_path = config["model"]["params"].get("base_model_path", None)
#         if base_unet_path and os.path.exists(base_unet_path):
#             unet.load_state_dict(load_file(base_unet_path))
#             logger.info("Loaded PromptDresser UNet weights.")
#         cloth_path = config["model"]["params"].get("cloth_encoder_path", None)
#         if cloth_path and (cloth_encoder is not None):
#             cloth_encoder.load_state_dict(load_file(cloth_path), strict=False)
#             logger.info("Loaded cloth encoder weights.")
# 
#     # Cihaza ve dtype'a ta≈üƒ±
#     for m in filter(None, [unet, vae, text_enc, text_enc_2, cloth_encoder]):
#         m.to(device, dtype=weight_dtype)
# 
#     # Hafƒ±za optimizasyonlarƒ±
#     unet.to(memory_format=torch.channels_last)
#     vae.to(memory_format=torch.channels_last)
# 
#     # Gradient checkpointing
#     try:
#         unet.enable_gradient_checkpointing()
#         logger.info("Gradient checkpointing enabled.")
#     except Exception:
#         logger.info("Gradient checkpointing not available; continuing.")
# 
#     # xFormers veya slicing
#     enabled_xformers = False
#     try:
#         unet.set_use_memory_efficient_attention_xformers(True)
#         enabled_xformers = True
#         logger.info("xFormers memory-efficient attention enabled.")
#     except Exception:
#         try:
#             # Bazƒ± diffusers s√ºr√ºmlerinde farklƒ± isimler
#             unet.enable_xformers_memory_efficient_attention()
#             enabled_xformers = True
#             logger.info("xFormers memory-efficient attention enabled (alt method).")
#         except Exception:
#             logger.info("xFormers not found; falling back to attention slicing.")
#             try:
#                 unet.set_attention_slice("max")
#             except Exception:
#                 pass
# 
#     # VAE tiling & slicing
#     try:
#         vae.enable_slicing()
#         vae.enable_tiling()
#         logger.info("VAE slicing/tiling enabled.")
#     except Exception:
#         logger.info("VAE slicing/tiling not available; continuing.")
# 
#     # Freeze base
#     unet.requires_grad_(False)
#     vae.requires_grad_(False)
#     text_enc.requires_grad_(False)
#     text_enc_2.requires_grad_(False)
#     if cloth_encoder is not None:
#         cloth_encoder.requires_grad_(False)
# 
#     # Reference attention control (cloth encoder baƒülƒ± ise)
#     reference_control_writer = None
#     reference_control_reader = None
#     if not detach_cloth and cloth_encoder is not None:
#         reference_control_writer = ReferenceAttentionControl(
#             cloth_encoder,
#             do_classifier_free_guidance=False,
#             mode="write",
#             fusion_blocks="midup",
#             batch_size=args.batch_size,
#             is_train=True,
#         )
#         reference_control_reader = ReferenceAttentionControl(
#             unet,
#             do_classifier_free_guidance=False,
#             mode="read",
#             fusion_blocks="midup",
#             batch_size=args.batch_size,
#             is_train=True,
#         )
# 
#     # -----------------------------------------------------------------
#     # LoRA injection (regex)
#     # -----------------------------------------------------------------
#     lora_rank  = int(args.lora_rank)
#     lora_alpha = int(args.lora_alpha)
# 
#     # Config'ten gelmezse makul SDXL-UNet attention projeksiyonlarƒ± (mid + son iki up)
#     target_patterns = (
#         config.get("lora", {}).get("target_modules") or [
#             r"mid_block\.attentions\.\d+\.transformer_blocks\.\d+\.attn\d\.to_(q|k|v|out\.0)$",
#             r"up_blocks\.(2|3)\.attentions\.\d+\.transformer_blocks\.\d+\.attn\d\.to_(q|k|v|out\.0)$",
#             # istersen down_blocks da ekleyebilirsin ama VRAM artar:
#             # r"down_blocks\.\d+\.attentions\.\d+\.transformer_blocks\.\d+\.attn\d\.to_(q|k|v|out\.0)$"
#         ]
#     )
# 
#     lora_layers = replace_linear_with_lora(unet, target_patterns, rank=lora_rank, alpha=lora_alpha)
# 
#     # Eƒüitim parametreleri (LoRA)
#     lora_params = [p for layer in lora_layers.values() for p in (layer.lora_A, layer.lora_B)]
#     if len(lora_params) == 0:
#         # Debug i√ßin √∂rnek Linear isimlerini logla
#         examples = [n for n, m in unet.named_modules() if isinstance(m, nn.Linear)]
#         logger.error(f"No LoRA layers injected. Example Linear names: {examples[:15]}")
#         raise RuntimeError("LoRA injection failed: no target modules matched your patterns.")
# 
#     trainable = sum(p.numel() for p in lora_params)
#     logger.info(f"LoRA injected: {len(lora_layers)} layers, {trainable:,} trainable params (rank={lora_rank}, alpha={lora_alpha}).")
# 
#     # -----------------------------------------------------------------
#     # Dataset / Dataloader
#     # -----------------------------------------------------------------
#     train_dataset = PromptDresserStyleDataset(
#         data_dir=args.data_dir,
#         mode="fine",
#         image_size=args.image_size
#     )
# 
#     # Az veri limit
#     if args.max_train_samples is not None and args.max_train_samples > 0:
#         if hasattr(train_dataset, "pairs"):
#             train_dataset.pairs = train_dataset.pairs[:args.max_train_samples]
#             logger.info(f"Limited dataset to {len(train_dataset.pairs)} samples for quick training.")
#         else:
#             logger.info("max_train_samples set, but dataset has no 'pairs' attribute; skipping trim.")
# 
#     style_dist = try_get_style_dist(train_dataset)
#     logger.info(f"Style distribution: {style_dist}")
# 
#     train_loader = DataLoader(
#         train_dataset,
#         batch_size=args.batch_size,
#         shuffle=True,
#         num_workers=0,
#         pin_memory=False,
#         drop_last=False
#     )
# 
#     # -----------------------------------------------------------------
#     # Optimizer (8-bit varsa)
#     # -----------------------------------------------------------------
#     try:
#         import bitsandbytes as bnb
#         optimizer = bnb.optim.AdamW8bit(
#             lora_params,
#             lr=args.learning_rate,
#             betas=(0.9, 0.95),
#             weight_decay=0.01,
#             eps=1e-6
#         )
#         logger.info("Using AdamW 8-bit (bitsandbytes).")
#     except Exception:
#         optimizer = torch.optim.AdamW(
#             lora_params,
#             lr=args.learning_rate,
#             betas=(0.9, 0.95),
#             weight_decay=0.01,
#             eps=1e-6
#         )
#         logger.info("bitsandbytes not found; using torch AdamW.")
# 
#     scaler = torch.cuda.amp.GradScaler(enabled=(weight_dtype == torch.float16))
# 
#     # -----------------------------------------------------------------
#     # Eƒüitim d√∂ng√ºs√º
#     # -----------------------------------------------------------------
#     global_step = 0
#     logger.info("***** Start Style-Enhanced LoRA Training *****")
#     logger.info(f"Examples={len(train_dataset)} | Epochs={args.num_train_epochs} | BS={args.batch_size} | Accum={args.gradient_accumulation_steps}")
#     logger.info(f"Detach cloth encoder = {detach_cloth} | mixed_precision={args.mixed_precision} | image_size={args.image_size}")
# 
#     total_steps = args.num_train_epochs * math.ceil(len(train_loader))
#     pbar = tqdm(total=total_steps, desc="Training", dynamic_ncols=True)
# 
#     unet.train()  # sadece LoRA parametreleri trainable zaten
# 
#     for epoch in range(args.num_train_epochs):
#         epoch_loss = 0.0
#         valid_steps = 0
# 
#         for step, batch in enumerate(train_loader):
# 
#             if not validate_batch(batch):
#                 continue
# 
#             # Cihaza ta≈üƒ±
#             person_img = batch["person"].to(device, dtype=weight_dtype, non_blocking=True)
#             mask       = batch["mask"].to(device, dtype=weight_dtype, non_blocking=True)
#             pose       = batch["pose"].to(device, dtype=weight_dtype, non_blocking=True)
#             cloth      = batch["cloth"].to(device, dtype=weight_dtype, non_blocking=True)
#             target_img = batch["target"].to(device, dtype=weight_dtype, non_blocking=True)
#             prompts    = batch["prompt"]
# 
#             bsz = person_img.shape[0]
# 
#             # ----- Latent encode (autocast + no_grad)
#             with torch.no_grad(), torch.autocast("cuda", dtype=weight_dtype, enabled=(weight_dtype != torch.float32)):
#                 person_latents = vae.encode(person_img).latent_dist.sample() * vae.config.scaling_factor
#                 target_latents = vae.encode(target_img).latent_dist.sample() * vae.config.scaling_factor
#                 cloth_latents  = vae.encode(cloth).latent_dist.sample() * vae.config.scaling_factor
# 
#                 # mask'ƒ± latent boyutuna indir
#                 mask_latents = torch.nn.functional.interpolate(
#                     mask, size=person_latents.shape[-2:], mode="nearest"
#                 )
# 
#                 # Text encoders
#                 text_in  = tokenizer(prompts, padding="max_length", max_length=77, truncation=True, return_tensors="pt").to(device)
#                 text_in2 = tokenizer_2(prompts, padding="max_length", max_length=77, truncation=True, return_tensors="pt").to(device)
# 
#                 text_emb     = text_enc(text_in.input_ids)[0]  # (bs, 77, d)
#                 te2_out      = text_enc_2(text_in2.input_ids)
#                 text_emb2    = te2_out.last_hidden_state      # (bs, 77, d2)
#                 pooled_text  = te2_out.text_embeds             # (bs, d2)
# 
#                 # SDXL encoder hidden states birle≈ütir
#                 encoder_hidden_states = torch.cat([text_emb, text_emb2], dim=-1)
# 
#                 # Cloth encoder (opsiyonel, VRAM i√ßin kapattƒ±k)
#                 if not detach_cloth and cloth_encoder is not None:
#                     time_ids_cloth = create_time_ids(bsz, device, weight_dtype)
#                     added_cond_kwargs_cloth = {'text_embeds': pooled_text, 'time_ids': time_ids_cloth}
#                     _ = cloth_encoder(
#                         cloth_latents,
#                         timestep=torch.zeros(bsz, device=device, dtype=torch.long),
#                         encoder_hidden_states=encoder_hidden_states,
#                         added_cond_kwargs=added_cond_kwargs_cloth,
#                         return_dict=False
#                     )[0]
# 
#             # ----- Diffusion step
#             with torch.autocast("cuda", dtype=weight_dtype, enabled=(weight_dtype != torch.float32)):
#                 noise = torch.randn_like(target_latents)
#                 timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=device).long()
#                 noisy_latents = noise_scheduler.add_noise(target_latents, noise, timesteps)
# 
#                 # UNet giri≈üi: (noisy, mask, person) concat
#                 unet_input = torch.cat([noisy_latents, mask_latents, person_latents], dim=1)
# 
#                 time_ids = create_time_ids(bsz, device, weight_dtype)
#                 added_cond_kwargs = {'text_embeds': pooled_text, 'time_ids': time_ids}
# 
#                 # ƒ∞leri ge√ßi≈ü
#                 model_pred = unet(
#                     sample=unet_input,
#                     timestep=timesteps,
#                     encoder_hidden_states=encoder_hidden_states,
#                     added_cond_kwargs=added_cond_kwargs,
#                     return_dict=True
#                 ).sample
# 
#                 loss = nn.functional.mse_loss(model_pred.float(), noise.float(), reduction="mean")
#                 loss = loss / args.gradient_accumulation_steps
# 
#             # Backward (GradScaler ile)
#             scaler.scale(loss).backward()
#             epoch_loss += loss.detach().float().item()
#             valid_steps += 1
# 
#             # Gradient accumulation
#             if (step + 1) % args.gradient_accumulation_steps == 0:
#                 # LoRA paramlarƒ±nda NaN guard
#                 for layer in lora_layers.values():
#                     if layer.lora_A.grad is not None and torch.isnan(layer.lora_A.grad).any():
#                         layer.lora_A.grad.zero_()
#                     if layer.lora_B.grad is not None and torch.isnan(layer.lora_B.grad).any():
#                         layer.lora_B.grad.zero_()
# 
#                 torch.nn.utils.clip_grad_norm_(lora_params, args.max_grad_norm)
#                 scaler.step(optimizer)
#                 scaler.update()
#                 optimizer.zero_grad(set_to_none=True)
# 
#                 global_step += 1
#                 pbar.update(1)
# 
#                 # Logging
#                 if global_step % args.logging_steps == 0 and valid_steps > 0:
#                     avg_loss = epoch_loss / valid_steps * args.gradient_accumulation_steps
#                     pbar.set_postfix({'loss': f'{avg_loss:.4f}', 'lr': f'{args.learning_rate:.2e}'})
#                     logger.info(f"Step {global_step}: loss={avg_loss:.4f}")
# 
#                 # Checkpoint
#                 if global_step % args.save_steps == 0:
#                     ckpt_dir = Path(args.output_dir) / f"style-checkpoint-{global_step}"
#                     ckpt_dir.mkdir(parents=True, exist_ok=True)
# 
#                     # Sadece LoRA aƒüƒ±rlƒ±klarƒ±
#                     lora_state = {}
#                     for name, layer in lora_layers.items():
#                         lora_state[f"{name}.lora_A"] = layer.lora_A.detach().cpu()
#                         lora_state[f"{name}.lora_B"] = layer.lora_B.detach().cpu()
# 
#                     torch.save({
#                         "lora_weights": lora_state,
#                         "optimizer": optimizer.state_dict(),
#                         "global_step": global_step,
#                         "epoch": epoch,
#                         "config": config,
#                         "style_distribution": style_dist,
#                         "lora_rank": lora_rank,
#                         "lora_alpha": lora_alpha
#                     }, ckpt_dir / "style_checkpoint.pt")
#                     logger.info(f"Saved checkpoint: {ckpt_dir}")
# 
#                 # VRAM leak √∂nleme
#                 del person_latents, target_latents, cloth_latents, mask_latents, unet_input, model_pred, noise
#                 torch.cuda.empty_cache()
# 
#             # batch tens√∂rlerini bƒ±rak
#             del person_img, mask, pose, cloth, target_img
#             # prompts CPU stringleri; sorun deƒüil
# 
#         if valid_steps > 0:
#             avg_epoch = epoch_loss / valid_steps * args.gradient_accumulation_steps
#             logger.info(f"Epoch {epoch+1}/{args.num_train_epochs} done | avg_loss={avg_epoch:.4f}")
# 
#     pbar.close()
# 
#     # -----------------------------------------------------------------
#     # Final LoRA aƒüƒ±rlƒ±klarƒ±
#     # -----------------------------------------------------------------
#     final_dir = Path(args.output_dir) / "final_style_lora"
#     final_dir.mkdir(parents=True, exist_ok=True)
# 
#     lora_state = {}
#     for name, layer in lora_layers.items():
#         lora_state[f"{name}.lora_A"] = layer.lora_A.detach().cpu()
#         lora_state[f"{name}.lora_B"] = layer.lora_B.detach().cpu()
# 
#     final_ckpt = {
#         "lora_weights": lora_state,
#         "config": config,
#         "style_distribution": style_dist,
#         "training_samples": len(train_dataset),
#         "lora_rank": lora_rank,
#         "lora_alpha": lora_alpha
#     }
#     torch.save(final_ckpt, final_dir / "style_lora_weights.pt")
# 
#     with open(final_dir / "style_config.yaml", "w") as f:
#         yaml.safe_dump(config, f)
# 
#     with open(final_dir / "style_info.txt", "w") as f:
#         f.write("STYLE-ENHANCED PROMPTDRESSER LoRA (VRAM-OPTIMIZED)\n")
#         f.write("===============================================\n\n")
#         f.write(f"Samples: {len(train_dataset)}\n")
#         f.write(f"Style dist: {style_dist}\n")
#         f.write(f"LoRA rank: {lora_rank} | alpha: {lora_alpha}\n")
#         f.write(f"Detach cloth encoder: {detach_cloth}\n")
#         f.write(f"xFormers: {enabled_xformers}\n")
# 
#     logger.info(f"Training complete. Final LoRA saved to: {final_dir}")
# 
# if __name__ == "__main__":
#     main()
#

!CUDA_VISIBLE_DEVICES=0 python /content/drive/MyDrive/PromptDresser/inference.py \
  --config_p "./configs/VITONHD.yaml" \
  --pretrained_unet_path "./checkpoints/VITONHD/model/pytorch_model.bin" \
  --lora_weights_path "./checkpoints/lorattraining_patternnew100s/final_pattern_lora/pattern_lora_weights.pt" \
  --lora_rank 16 --lora_alpha 32 \
  --save_name lora_test100 \
  --skip_unpaired \
  --s_idx 0 --e_idx 100

!python evaluation.py \
  --gt_dir ./DATA/zalando-hd-resized/test_fine/image \
  --pred_dir ./sampled_images/lora_test100/paired \
  --enhanced \
  --model_path ./checkpoints/VITONHD/model/pytorch_model.bin \
  --text_pairs_json test_gpt4o.json

!CUDA_VISIBLE_DEVICES=0 python /content/drive/MyDrive/PromptDresser/inference.py \
  --config_p "./configs/VITONHD.yaml" \
  --pretrained_unet_path "./checkpoints/VITONHD/model/pytorch_model.bin" \
  --lora_weights_path "./checkpoints/lorattraining_patternnew100s/final_pattern_lora/pattern_lora_weights.pt" \
  --lora_rank 16 --lora_alpha 32 \
  --save_name NEW_test8_LORA \
  --skip_unpaired \
  --s_idx 0 --e_idx 100

!python train_pattern_lora.py \
  --config configs/VITONHD_lora.yaml \
  --data_dir ./DATA/zalando-hd-resized \
  --output_dir ./checkpoints/lorattraining_patternnew100s \
  --num_train_epochs 4 \
  --batch_size 1 \
  --gradient_accumulation_steps 8 \
  --learning_rate 5e-5 \
  --max_train_samples 100 \
  --image_size 384 \
  --mixed_precision bf16

# Commented out IPython magic to ensure Python compatibility.
# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-
# %%writefile /content/drive/MyDrive/PromptDresser/train_pattern_lora.py
# """
# PATTERN-ONLY LORA TRAINING FOR PROMPTDRESSER (NO STYLE FIELDS)
# - Style/season (summer/winter vs.) tamamen kaldƒ±rƒ±ldƒ±.
# - Sadece (person, cloth, target, mask, pose) e≈üle≈ümeleri ile √ßalƒ±≈üƒ±r.
# - Dataset'in d√∂nd√ºrd√ºƒü√º `prompt` alanƒ± aynen kullanƒ±lƒ±r.
# - VRAM dostu optimizasyonlar korunmu≈ütur (fp16/bf16, grad checkpointing, xFormers/slicing, VAE tiling, LoRA regex).
# """
# 
# import os
# import re
# import sys
# import yaml
# import math
# import torch
# import torch.nn as nn
# from torch.utils.data import DataLoader
# import argparse
# import logging
# from pathlib import Path
# from tqdm import tqdm
# 
# from diffusers import AutoencoderKL, DDPMScheduler
# from transformers import CLIPTextModel, CLIPTokenizer, CLIPTextModelWithProjection
# 
# # PromptDresser repo yollarƒ±
# sys.path.append('/content/drive/MyDrive/PromptDresser')
# 
# # Dataset (prompts dahil, ama style yok)
# from promptdresser_style_dataset import PromptDresserStyleDataset
# from promptdresser.models.unet import UNet2DConditionModel
# from promptdresser.models.cloth_encoder import ClothEncoder
# from promptdresser.models.mutual_self_attention import ReferenceAttentionControl
# from promptdresser.utils import load_file
# 
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger("pattern_lora")
# 
# class LoRALinear(nn.Module):
#     def __init__(self, original_linear: nn.Linear, rank=8, alpha=16):
#         super().__init__()
#         self.original_linear = original_linear
#         self.rank = rank
#         self.alpha = alpha
#         self.scaling = alpha / max(1, rank)
#         device = original_linear.weight.device
#         dtype = original_linear.weight.dtype
#         self.lora_A = nn.Parameter(torch.zeros(original_linear.in_features, rank, device=device, dtype=dtype))
#         self.lora_B = nn.Parameter(torch.zeros(rank, original_linear.out_features, device=device, dtype=dtype))
#         nn.init.normal_(self.lora_A, mean=0.0, std=0.01)
#         nn.init.zeros_(self.lora_B)
# 
#     def forward(self, x, scale: float = 1.0, **kwargs):
#         out = self.original_linear(x)
#         if self.training or scale != 0:
#             x_c = torch.clamp(x, -10, 10)
#             out = out + (x_c @ self.lora_A @ self.lora_B) * self.scaling * scale
#         return out
# 
# def replace_linear_with_lora(model: nn.Module, target_patterns, rank=8, alpha=16):
#     compiled = [re.compile(p) for p in target_patterns]
#     lora_layers = {}
#     for name, module in list(model.named_modules()):
#         if isinstance(module, nn.Linear) and any(p.search(name) for p in compiled):
#             *parents, attr = name.split(".")
#             parent = model
#             for p in parents:
#                 parent = getattr(parent, p)
#             lora_layer = LoRALinear(module, rank=rank, alpha=alpha)
#             setattr(parent, attr, lora_layer)
#             lora_layers[name] = lora_layer
#     return lora_layers
# 
# def create_time_ids(batch_size, device, dtype):
#     original_size = (1024, 768)
#     crops_coords_top_left = (0, 0)
#     target_size = (1024, 768)
#     time_ids = torch.tensor([
#         original_size[0], original_size[1],
#         crops_coords_top_left[0], crops_coords_top_left[1],
#         target_size[0], target_size[1]
#     ], dtype=torch.float32).unsqueeze(0)
#     return time_ids.repeat(batch_size, 1).to(device=device, dtype=dtype)
# 
# def parse_args():
#     ap = argparse.ArgumentParser(description="Pattern-Only LoRA Training for PromptDresser (VRAM-optimized)")
#     ap.add_argument("--config", type=str, required=True, help="./configs/VITONHD_lora.yaml")
#     ap.add_argument("--data_dir", type=str, default="./DATA/zalando-hd-resized", help="Dataset klas√∂r√º")
#     ap.add_argument("--output_dir", type=str, default="./checkpoints/pattern_lora_training", help="Checkpoint klas√∂r√º")
#     ap.add_argument("--num_train_epochs", type=int, default=3)
#     ap.add_argument("--batch_size", type=int, default=1)
#     ap.add_argument("--gradient_accumulation_steps", type=int, default=8)
#     ap.add_argument("--learning_rate", type=float, default=5e-5)
#     ap.add_argument("--max_grad_norm", type=float, default=0.5)
#     ap.add_argument("--max_train_samples", type=int, default=0)
#     ap.add_argument("--image_size", type=int, default=384)
#     ap.add_argument("--lora_rank", type=int, default=16)
#     ap.add_argument("--lora_alpha", type=int, default=32)
#     ap.add_argument("--save_steps", type=int, default=100)
#     ap.add_argument("--logging_steps", type=int, default=10)
#     ap.add_argument("--mixed_precision", type=str, default="fp16", choices=["no", "fp16", "bf16"])
#     ap.add_argument("--lora_scale", type=float, default=1.0)
#     return ap.parse_args()
# 
# def main():
#     args = parse_args()
#     with open(args.config, "r") as f:
#         config = yaml.safe_load(f)
# 
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# 
#     if args.mixed_precision == "bf16":
#         weight_dtype = torch.bfloat16
#     elif args.mixed_precision == "fp16":
#         weight_dtype = torch.float16
#     else:
#         weight_dtype = torch.float32
# 
#     os.makedirs(args.output_dir, exist_ok=True)
# 
#     logger.info("Loading base models‚Ä¶")
#     init_model_path = "./pretrained_models/stable-diffusion-xl-1.0-inpainting-0.1"
#     init_vae_path   = "./pretrained_models/sdxl-vae-fp16-fix"
#     init_cloth_encoder_path = "./pretrained_models/stable-diffusion-xl-base-1.0"
# 
#     noise_scheduler = DDPMScheduler.from_pretrained(init_model_path, subfolder="scheduler")
#     tokenizer   = CLIPTokenizer.from_pretrained(init_model_path, subfolder="tokenizer")
#     text_enc    = CLIPTextModel.from_pretrained(init_model_path, subfolder="text_encoder")
#     tokenizer_2 = CLIPTokenizer.from_pretrained(init_model_path, subfolder="tokenizer_2")
#     text_enc_2  = CLIPTextModelWithProjection.from_pretrained(init_model_path, subfolder="text_encoder_2")
# 
#     vae  = AutoencoderKL.from_pretrained(init_vae_path)
#     unet = UNet2DConditionModel.from_pretrained(init_model_path, subfolder="unet")
# 
#     detach_cloth = bool(config.get("detach_cloth_encoder", True))
#     cloth_encoder = None
#     if not detach_cloth:
#         cloth_encoder = ClothEncoder.from_pretrained(init_cloth_encoder_path, subfolder="unet")
# 
#     if "model" in config and "params" in config["model"]:
#         base_unet_path = config["model"]["params"].get("base_model_path", None)
#         if base_unet_path and os.path.exists(base_unet_path):
#             unet.load_state_dict(load_file(base_unet_path))
#             logger.info("Loaded PromptDresser UNet weights.")
#         cloth_path = config["model"]["params"].get("cloth_encoder_path", None)
#         if cloth_path and (cloth_encoder is not None):
#             cloth_encoder.load_state_dict(load_file(cloth_path), strict=False)
#             logger.info("Loaded cloth encoder weights.")
# 
#     for m in filter(None, [unet, vae, text_enc, text_enc_2, cloth_encoder]):
#         m.to(device, dtype=weight_dtype)
# 
#     unet.to(memory_format=torch.channels_last)
#     vae.to(memory_format=torch.channels_last)
# 
#     try:
#         unet.enable_gradient_checkpointing()
#         logger.info("Gradient checkpointing enabled.")
#     except Exception:
#         logger.info("Gradient checkpointing not available; continuing.")
# 
#     try:
#         unet.set_use_memory_efficient_attention_xformers(True)
#         logger.info("xFormers attention enabled.")
#     except Exception:
#         try:
#             unet.enable_xformers_memory_efficient_attention()
#             logger.info("xFormers attention enabled (alt method).")
#         except Exception:
#             logger.info("xFormers not found; using attention slicing.")
#             try:
#                 unet.set_attention_slice("max")
#             except Exception:
#                 pass
# 
#     try:
#         vae.enable_slicing(); vae.enable_tiling()
#         logger.info("VAE slicing/tiling enabled.")
#     except Exception:
#         logger.info("VAE slicing/tiling not available; continuing.")
# 
#     unet.requires_grad_(False)
#     vae.requires_grad_(False)
#     text_enc.requires_grad_(False)
#     text_enc_2.requires_grad_(False)
#     if cloth_encoder is not None:
#         cloth_encoder.requires_grad_(False)
# 
#     reference_control_writer = None
#     reference_control_reader = None
#     if not detach_cloth and cloth_encoder is not None:
#         reference_control_writer = ReferenceAttentionControl(
#             cloth_encoder, do_classifier_free_guidance=False, mode="write", fusion_blocks="midup",
#             batch_size=1, is_train=True,
#         )
#         reference_control_reader = ReferenceAttentionControl(
#             unet, do_classifier_free_guidance=False, mode="read", fusion_blocks="midup",
#             batch_size=1, is_train=True,
#         )
# 
#     lora_rank  = int(args.lora_rank)
#     lora_alpha = int(args.lora_alpha)
# 
#     target_patterns = (
#         config.get("lora", {}).get("target_modules") or [
#             r"mid_block\.attentions\.\d+\.transformer_blocks\.\d+\.attn\d\.to_(q|k|v|out\.0)$",
#             r"up_blocks\.(2|3)\.attentions\.\d+\.transformer_blocks\.\d+\.attn\d\.to_(q|k|v|out\.0)$",
#         ]
#     )
# 
#     lora_layers = replace_linear_with_lora(unet, target_patterns, rank=lora_rank, alpha=lora_alpha)
#     lora_params = [p for layer in lora_layers.values() for p in (layer.lora_A, layer.lora_B)]
#     if len(lora_params) == 0:
#         examples = [n for n, m in unet.named_modules() if isinstance(m, nn.Linear)]
#         logger.error(f"No LoRA layers injected. Example Linear names: {examples[:15]}")
#         raise RuntimeError("LoRA injection failed: no target modules matched your patterns.")
# 
#     train_dataset = PromptDresserStyleDataset(
#         data_dir=args.data_dir,
#         mode="fine",
#         image_size=args.image_size
#     )
# 
#     if args.max_train_samples and args.max_train_samples > 0:
#         if hasattr(train_dataset, "pairs"):
#             train_dataset.pairs = train_dataset.pairs[:args.max_train_samples]
#             logger.info(f"Limited dataset to {len(train_dataset.pairs)} samples.")
# 
#     train_loader = DataLoader(
#         train_dataset,
#         batch_size=args.batch_size,
#         shuffle=True,
#         num_workers=0,
#         pin_memory=False,
#         drop_last=False
#     )
# 
#     try:
#         import bitsandbytes as bnb
#         optimizer = bnb.optim.AdamW8bit(
#             lora_params, lr=args.learning_rate, betas=(0.9, 0.95), weight_decay=0.01, eps=1e-6
#         )
#         logger.info("Using AdamW 8-bit (bitsandbytes).")
#     except Exception:
#         optimizer = torch.optim.AdamW(
#             lora_params, lr=args.learning_rate, betas=(0.9, 0.95), weight_decay=0.01, eps=1e-6
#         )
#         logger.info("bitsandbytes not found; using torch AdamW.")
# 
#     scaler = torch.cuda.amp.GradScaler(enabled=(weight_dtype == torch.float16))
# 
#     global_step = 0
#     logger.info("***** Start Pattern-Only LoRA Training *****")
#     logger.info(f"Examples={len(train_dataset)} | Epochs={args.num_train_epochs} | BS={args.batch_size} | Accum={args.gradient_accumulation_steps}")
# 
#     total_steps = args.num_train_epochs * math.ceil(len(train_loader) / max(1, args.gradient_accumulation_steps))
#     pbar = tqdm(total=total_steps, desc="Training", dynamic_ncols=True)
# 
#     unet.train()
# 
#     for epoch in range(args.num_train_epochs):
#         epoch_loss = 0.0
#         valid_steps = 0
#         for step, batch in enumerate(train_loader):
#             person_img = batch["person"].to(device, dtype=weight_dtype, non_blocking=True)
#             mask       = batch["mask"].to(device, dtype=weight_dtype, non_blocking=True)
#             pose       = batch.get("pose", None)
#             if torch.is_tensor(pose):
#                 pose = pose.to(device, dtype=weight_dtype, non_blocking=True)
#             cloth      = batch["cloth"].to(device, dtype=weight_dtype, non_blocking=True)
#             target_img = batch["target"].to(device, dtype=weight_dtype, non_blocking=True)
# 
#             bsz = person_img.shape[0]
#             prompts = batch.get("prompt", [""] * bsz)
# 
#             with torch.no_grad(), torch.autocast("cuda", dtype=weight_dtype, enabled=(weight_dtype != torch.float32)):
#                 person_latents = vae.encode(person_img).latent_dist.sample() * vae.config.scaling_factor
#                 target_latents = vae.encode(target_img).latent_dist.sample() * vae.config.scaling_factor
#                 cloth_latents  = vae.encode(cloth).latent_dist.sample() * vae.config.scaling_factor
#                 mask_latents = torch.nn.functional.interpolate(mask, size=person_latents.shape[-2:], mode="nearest")
# 
#                 text_in  = tokenizer(prompts, padding="max_length", max_length=77, truncation=True, return_tensors="pt").to(device)
#                 text_in2 = tokenizer_2(prompts, padding="max_length", max_length=77, truncation=True, return_tensors="pt").to(device)
#                 text_emb     = text_enc(text_in.input_ids)[0]
#                 te2_out      = text_enc_2(text_in2.input_ids)
#                 text_emb2    = te2_out.last_hidden_state
#                 pooled_text  = te2_out.text_embeds
#                 encoder_hidden_states = torch.cat([text_emb, text_emb2], dim=-1)
# 
#                 if not detach_cloth and cloth_encoder is not None:
#                     time_ids_cloth = create_time_ids(bsz, device, weight_dtype)
#                     added_cond_kwargs_cloth = {'text_embeds': pooled_text, 'time_ids': time_ids_cloth}
#                     _ = cloth_encoder(
#                         cloth_latents,
#                         timestep=torch.zeros(bsz, device=device, dtype=torch.long),
#                         encoder_hidden_states=encoder_hidden_states,
#                         added_cond_kwargs=added_cond_kwargs_cloth,
#                         return_dict=False
#                     )[0]
# 
#             with torch.autocast("cuda", dtype=weight_dtype, enabled=(weight_dtype != torch.float32)):
#                 noise = torch.randn_like(target_latents)
#                 timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=device).long()
#                 noisy_latents = noise_scheduler.add_noise(target_latents, noise, timesteps)
# 
#                 unet_input = torch.cat([noisy_latents, mask_latents, person_latents], dim=1)
#                 time_ids = create_time_ids(bsz, device, weight_dtype)
#                 added_cond_kwargs = {'text_embeds': pooled_text, 'time_ids': time_ids}
#                 model_pred = unet(
#                     sample=unet_input,
#                     timestep=timesteps,
#                     encoder_hidden_states=encoder_hidden_states,
#                     added_cond_kwargs=added_cond_kwargs,
#                     return_dict=True
#                 ).sample
# 
#                 loss = nn.functional.mse_loss(model_pred.float(), noise.float(), reduction="mean")
#                 loss = loss / args.gradient_accumulation_steps
# 
#             scaler.scale(loss).backward()
#             epoch_loss += loss.detach().float().item()
#             valid_steps += 1
# 
#             if (step + 1) % args.gradient_accumulation_steps == 0:
#                 for layer in lora_layers.values():
#                     if layer.lora_A.grad is not None and torch.isnan(layer.lora_A.grad).any():
#                         layer.lora_A.grad.zero_()
#                     if layer.lora_B.grad is not None and torch.isnan(layer.lora_B.grad).any():
#                         layer.lora_B.grad.zero_()
# 
#                 torch.nn.utils.clip_grad_norm_(lora_params, args.max_grad_norm)
#                 scaler.step(optimizer)
#                 scaler.update()
#                 optimizer.zero_grad(set_to_none=True)
# 
#                 global_step += 1
#                 pbar.update(1)
# 
#                 if global_step % args.logging_steps == 0 and valid_steps > 0:
#                     avg_loss = epoch_loss / valid_steps * args.gradient_accumulation_steps
#                     pbar.set_postfix({'loss': f'{avg_loss:.4f}', 'lr': f'{args.learning_rate:.2e}'})
#                     logger.info(f"Step {global_step}: loss={avg_loss:.4f}")
# 
#                 if global_step % args.save_steps == 0:
#                     ckpt_dir = Path(args.output_dir) / f"pattern-checkpoint-{global_step}"
#                     ckpt_dir.mkdir(parents=True, exist_ok=True)
#                     lora_state = {}
#                     for name, layer in lora_layers.items():
#                         lora_state[f"{name}.lora_A"] = layer.lora_A.detach().cpu()
#                         lora_state[f"{name}.lora_B"] = layer.lora_B.detach().cpu()
#                     torch.save({
#                         "lora_weights": lora_state,
#                         "optimizer": optimizer.state_dict(),
#                         "global_step": global_step,
#                         "epoch": epoch,
#                         "config": config,
#                         "lora_rank": lora_rank,
#                         "lora_alpha": lora_alpha
#                     }, ckpt_dir / "pattern_checkpoint.pt")
#                     logger.info(f"Saved checkpoint: {ckpt_dir}")
# 
#                 # Temizlik & VRAM guard
#                 del person_latents, target_latents, cloth_latents, mask_latents, unet_input, model_pred, noise
#                 torch.cuda.empty_cache()
# 
#             # batch tens√∂rlerini bƒ±rak
#             del person_img, mask, cloth, target_img
# 
#         if valid_steps > 0:
#             avg_epoch = epoch_loss / valid_steps * args.gradient_accumulation_steps
#             logger.info(f"Epoch {epoch+1}/{args.num_train_epochs} done | avg_loss={avg_epoch:.4f}")
# 
#     pbar.close()
# 
#     # Final LoRA aƒüƒ±rlƒ±klarƒ±
#     final_dir = Path(args.output_dir) / "final_pattern_lora"
#     final_dir.mkdir(parents=True, exist_ok=True)
# 
#     lora_state = {}
#     for name, layer in lora_layers.items():
#         lora_state[f"{name}.lora_A"] = layer.lora_A.detach().cpu()
#         lora_state[f"{name}.lora_B"] = layer.lora_B.detach().cpu()
# 
#     final_ckpt = {
#         "lora_weights": lora_state,
#         "config": config,
#         "training_samples": len(train_dataset),
#         "lora_rank": lora_rank,
#         "lora_alpha": lora_alpha
#     }
#     torch.save(final_ckpt, final_dir / "pattern_lora_weights.pt")
# 
#     with open(final_dir / "pattern_info.txt", "w") as f:
#         f.write("PATTERN-ONLY PROMPTDRESSER LORA (VRAM-OPTIMIZED)\n")
#         f.write("===============================================\n\n")
#         f.write(f"Samples: {len(train_dataset)}\n")
#         f.write(f"LoRA rank: {lora_rank} | alpha: {lora_alpha}\n")
#         f.write(f"Detach cloth encoder: {detach_cloth}\n")
# 
#     logger.info(f"Training complete. Final LoRA saved to: {final_dir}")
# 
# if __name__ == "__main__":
#     main()
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/drive/MyDrive/PromptDresser/promptdresser_style_dataset.py
# import os
# import json
# import numpy as np
# from PIL import Image
# import torch
# from torch.utils.data import Dataset
# from torchvision import transforms
# 
# class PromptDresserStyleDataset(Dataset):
#     """
#     MINIMAL DATASET (NO STYLE, NO AUTO-FILTER)
#     - Hi√ßbir otomatik desen/keyword/JSON filtresi YOK.
#     - Senin verdiƒüin pairs dosyasƒ±nƒ± (√∂rn. test_pairs.txt) AYNEN okur.
#     - Prompt varsa JSON'dan alƒ±r; yoksa bo≈ü string d√∂ner.
#     """
#     def __init__(self, data_dir, mode='fine', image_size=512, return_pose=False, pairs_file='test_pairs.txt', prompts_json=None):
#         self.data_dir = data_dir
#         self.mode = mode
#         self.image_size = image_size
#         self.return_pose = return_pose
#         self.pairs_file = pairs_file
#         self.prompts = self._load_prompts(prompts_json)
# 
#         self.pairs = self._load_pairs()
# 
#         # FIX: Use Image.Resampling instead of InterpolationMode
#         self.transform = transforms.Compose([
#             transforms.Resize((image_size, image_size), interpolation=Image.Resampling.BILINEAR),
#             transforms.ToTensor(),
#             transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),
#         ])
# 
#     # -----------------------------
#     # I/O
#     # -----------------------------
#     def _load_prompts(self, prompts_json):
#         if not prompts_json:
#             return {}
#         p = prompts_json if os.path.isabs(prompts_json) else os.path.join(self.data_dir, prompts_json)
#         if not os.path.exists(p):
#             print(f"[WARN] prompt json not found: {p}")
#             return {}
#         try:
#             with open(p, 'r') as f:
#                 data = json.load(f)
#             print(f"[INFO] Loaded prompts from: {os.path.basename(p)}")
#             return data
#         except Exception as e:
#             print(f"[WARN] failed to load prompts: {e}")
#             return {}
# 
#     def _load_pairs(self):
#         pairs_path = os.path.join(self.data_dir, self.pairs_file)
#         pairs = []
#         if not os.path.exists(pairs_path):
#             print(f"[WARN] pairs file not found: {pairs_path}")
#             return pairs
#         with open(pairs_path, 'r') as f:
#             for line in f:
#                 line = line.strip()
#                 if not line:
#                     continue
#                 toks = line.split()
#                 if len(toks) < 2:
#                     continue
#                 person_img, cloth_img = toks[0], toks[1]
#                 pairs.append((person_img, cloth_img))
#         print(f"[INFO] Loaded {len(pairs)} pairs from {self.pairs_file}")
#         return pairs
# 
#     # -----------------------------
#     # Loaders
#     # -----------------------------
#     def _load_image(self, path):
#         try:
#             img = Image.open(path).convert('RGB')
#         except Exception as e:
#             print(f"[WARN] Error loading image {path}: {e}; using black dummy.")
#             img = Image.new('RGB', (self.image_size, self.image_size), color='black')
#         return self.transform(img)
# 
#     def _load_mask(self, path):
#         try:
#             m = Image.open(path).convert('L')
#             # FIX: Use Image.Resampling.NEAREST instead of IM.NEAREST
#             m = m.resize((self.image_size, self.image_size), resample=Image.Resampling.NEAREST)
#             arr = np.array(m, dtype=np.float32) / 255.0
#             return torch.from_numpy(arr).unsqueeze(0)
#         except Exception as e:
#             print(f"[WARN] Error loading mask {path}: {e}; using 0.5 dummy.")
#             return torch.full((1, self.image_size, self.image_size), 0.5, dtype=torch.float32)
# 
#     def _find_mask_path(self, person_id):
#         tries = [
#             os.path.join(self.data_dir, 'test_coarse', 'agnostic-mask', f'{person_id}_mask.png'),
#             os.path.join(self.data_dir, f'test_{self.mode}', 'agnostic-mask', f'{person_id}_mask.png'),
#             os.path.join(self.data_dir, 'test_coarse', 'agnostic-mask', f'{person_id}.png'),
#             os.path.join(self.data_dir, f'test_{self.mode}', 'agnostic-mask', f'{person_id}.png'),
#         ]
#         for p in tries:
#             if os.path.exists(p):
#                 return p
#         return None
# 
#     def _find_pose_path(self, person_id):
#         tries = [
#             os.path.join(self.data_dir, f'test_{self.mode}', 'image-densepose', f'{person_id}.png'),
#             os.path.join(self.data_dir, f'test_{self.mode}', 'image-densepose', f'{person_id}.jpg'),
#             os.path.join(self.data_dir, f'test_{self.mode}', 'openpose_img', f'{person_id}_rendered.png'),
#         ]
#         for p in tries:
#             if os.path.exists(p):
#                 return p
#         return None
# 
#     # -----------------------------
#     # PyTorch API
#     # -----------------------------
#     def __getitem__(self, idx):
#         person_img_name, cloth_img_name = self.pairs[idx]
#         person_id = os.path.splitext(person_img_name)[0]
#         cloth_id  = os.path.splitext(cloth_img_name)[0]
# 
#         person_path = os.path.join(self.data_dir, f'test_{self.mode}', 'image', person_img_name)
#         cloth_path  = os.path.join(self.data_dir, f'test_{self.mode}', 'cloth', cloth_img_name)
# 
#         person_img = self._load_image(person_path)
#         cloth_img  = self._load_image(cloth_path)
#         target_img = person_img  # clone yok ‚Äî daha az RAM/VRAM
# 
#         mask_path = self._find_mask_path(person_id)
#         mask = self._load_mask(mask_path) if mask_path else torch.full(
#             (1, self.image_size, self.image_size), 0.5, dtype=torch.float32
#         )
# 
#         if self.return_pose:
#             pose_path = self._find_pose_path(person_id)
#             pose_img = self._load_image(pose_path) if pose_path else person_img
#         else:
#             pose_img = torch.zeros(1, self.image_size, self.image_size, dtype=torch.float32)
# 
#         # Prompt: varsa JSON'dan; yoksa bo≈ü string
#         prompt = ""
#         if self.prompts:
#             data = self.prompts.get(person_id) or self.prompts.get(cloth_id)
#             if isinstance(data, dict):
#                 prompt = data.get('prompt') or data.get('caption') or ""
#             elif isinstance(data, str):
#                 prompt = data
# 
#         return {
#             'person': person_img,
#             'cloth': cloth_img,
#             'target': target_img,
#             'mask': mask,
#             'pose': pose_img,
#             'prompt': prompt,
#             'person_id': person_id,
#             'cloth_id': cloth_id
#         }
# 
#     def __len__(self):
#         return len(self.pairs)
# 
# if __name__ == "__main__":
#     ds = PromptDresserStyleDataset(
#         data_dir="./DATA/zalando-hd-resized",
#         mode="fine",
#         image_size=512,
#         return_pose=False,
#         pairs_file='test_pairs.txt',
#         prompts_json=None
#     )
#     print(f"Dataset size: {len(ds)}")
#     for i in range(min(3, len(ds))):
#         s = ds[i]
#         print(f"[{i}] {s['person_id']} -> {s['cloth_id']} | prompt[:60]={s['prompt'][:60]!r}")